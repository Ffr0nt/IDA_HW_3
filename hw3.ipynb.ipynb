{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKdcvMtSMpSg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "#Майнор ИАД. Домашнее задание 3. .\n",
    "\n",
    "В этом задании вы напишете и обучите свой собственный YOLO детектор. Нужно будет разобраться со статьей: понять какого формата должна быть обучающая пара (x, y), как перевести лосс из математической формулы в питоновский код - ну и конечно понять и реализовать саму архитектуру модели.\n",
    "\n",
    "Выборка на котрой мы будем обучать модель состоит из разнообразных фотографий яблок, бананов и апельсинов. Данные скачиваем [отсюда](https://drive.google.com/file/d/1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3/view?usp=sharing).\n",
    "\n",
    "Баллы за ДЗ распределены следующим образом: \n",
    "- Выборка для YoloV1 - 2 балла\n",
    "- YOLO модель - 2 балла\n",
    "- YOLO Loss - 3 балла\n",
    "- Вспомогательные функции - 2 балла\n",
    "- Обучение и расчет метрик - 2 балла\n",
    "\n",
    "Для построения и обучения можно использовать как pytorch, так и pytorch-lightning.\n",
    "\n",
    "Да-да, баллов в сумме получается 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TY4niK1xMpSg",
    "outputId": "1810d5cf-5d4d-4b5b-db31-5a220ee2b74d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (0.13.0)\r\n",
      "Requirement already satisfied: pytorch-lightning in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (1.8.3.post1)\r\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (6.0)\r\n",
      "Requirement already satisfied: tensorboardX>=2.2 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (2.5.1)\r\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (2022.10.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (1.21.5)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (4.3.0)\r\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (4.64.1)\r\n",
      "Requirement already satisfied: torch>=1.9.* in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (1.13.0)\r\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (21.3)\r\n",
      "Requirement already satisfied: lightning-utilities==0.3.* in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (0.3.0)\r\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (0.11.0)\r\n",
      "Requirement already satisfied: fire in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from lightning-utilities==0.3.*->pytorch-lightning) (0.4.0)\r\n",
      "Requirement already satisfied: requests in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.28.1)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\r\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from tensorboardX>=2.2->pytorch-lightning) (3.20.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.2)\r\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.1)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.1.0)\r\n",
      "Requirement already satisfied: termcolor in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from fire->lightning-utilities==0.3.*->pytorch-lightning) (1.1.0)\r\n",
      "Requirement already satisfied: six in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from fire->lightning-utilities==0.3.*->pytorch-lightning) (1.16.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.12)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.9.24)\r\n"
     ]
    }
   ],
   "source": [
    "# Данная библиотека понадобится нам, чтобы обработать разметку\n",
    "! pip install xmltodict pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNSQ7FNss30F",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Скачаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lN1dE6eY7PjV",
    "outputId": "beb005ec-2b9b-42ab-c294-127ee4ec4b39",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 64\r\n",
      "drwxr-xr-x  4 fedor  staff    128 Dec  1 12:27 \u001B[34m__MACOSX\u001B[m\u001B[m\r\n",
      "drwx------  5 fedor  staff    160 Nov 26 11:32 \u001B[34mdata\u001B[m\u001B[m\r\n",
      "-rw-r--r--  1 fedor  staff  31386 Dec  1 12:27 hw3.ipynb.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!wget --quiet --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://drive.google.com/uc?export=download&id=1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3\" -O data.zip && rm -rf /tmp/cookies.txt\n",
    "!unzip -q data.zip\n",
    "!rm data.zip\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ep38vdW_s-Rz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Посмотрим как выглядит один из файлов разметки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OqwaHl3ntBaN",
    "outputId": "77ce5c63-e716-4a57-90a9-2179534fa789",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<annotation>\r\n",
      "\t<folder>train</folder>\r\n",
      "\t<filename>apple_3.jpg</filename>\r\n",
      "\t<path>C:\\tensorflow1\\models\\research\\object_detection\\images\\train\\apple_3.jpg</path>\r\n",
      "\t<source>\r\n",
      "\t\t<database>Unknown</database>\r\n",
      "\t</source>\r\n",
      "\t<size>\r\n",
      "\t\t<width>1000</width>\r\n",
      "\t\t<height>708</height>\r\n",
      "\t\t<depth>3</depth>\r\n",
      "\t</size>\r\n",
      "\t<segmented>0</segmented>\r\n",
      "\t<object>\r\n",
      "\t\t<name>apple</name>\r\n",
      "\t\t<pose>Unspecified</pose>\r\n",
      "\t\t<truncated>1</truncated>\r\n",
      "\t\t<difficult>0</difficult>\r\n",
      "\t\t<bndbox>\r\n",
      "\t\t\t<xmin>584</xmin>\r\n",
      "\t\t\t<ymin>438</ymin>\r\n",
      "\t\t\t<xmax>867</xmax>\r\n",
      "\t\t\t<ymax>708</ymax>\r\n",
      "\t\t</bndbox>\r\n",
      "\t</object>\r\n",
      "\t<object>\r\n",
      "\t\t<name>apple</name>\r\n",
      "\t\t<pose>Unspecified</pose>\r\n",
      "\t\t<truncated>0</truncated>\r\n",
      "\t\t<difficult>0</difficult>\r\n",
      "\t\t<bndbox>\r\n",
      "\t\t\t<xmin>492</xmin>\r\n",
      "\t\t\t<ymin>141</ymin>\r\n",
      "\t\t\t<xmax>740</xmax>\r\n",
      "\t\t\t<ymax>394</ymax>\r\n",
      "\t\t</bndbox>\r\n",
      "\t</object>\r\n",
      "\t<object>\r\n",
      "\t\t<name>apple</name>\r\n",
      "\t\t<pose>Unspecified</pose>\r\n",
      "\t\t<truncated>0</truncated>\r\n",
      "\t\t<difficult>0</difficult>\r\n",
      "\t\t<bndbox>\r\n",
      "\t\t\t<xmin>176</xmin>\r\n",
      "\t\t\t<ymin>199</ymin>\r\n",
      "\t\t\t<xmax>490</xmax>\r\n",
      "\t\t\t<ymax>466</ymax>\r\n",
      "\t\t</bndbox>\r\n",
      "\t</object>\r\n",
      "\t<object>\r\n",
      "\t\t<name>apple</name>\r\n",
      "\t\t<pose>Unspecified</pose>\r\n",
      "\t\t<truncated>0</truncated>\r\n",
      "\t\t<difficult>0</difficult>\r\n",
      "\t\t<bndbox>\r\n",
      "\t\t\t<xmin>367</xmin>\r\n",
      "\t\t\t<ymin>17</ymin>\r\n",
      "\t\t\t<xmax>619</xmax>\r\n",
      "\t\t\t<ymax>240</ymax>\r\n",
      "\t\t</bndbox>\r\n",
      "\t</object>\r\n",
      "\t<object>\r\n",
      "\t\t<name>apple</name>\r\n",
      "\t\t<pose>Unspecified</pose>\r\n",
      "\t\t<truncated>0</truncated>\r\n",
      "\t\t<difficult>0</difficult>\r\n",
      "\t\t<bndbox>\r\n",
      "\t\t\t<xmin>642</xmin>\r\n",
      "\t\t\t<ymin>35</ymin>\r\n",
      "\t\t\t<xmax>907</xmax>\r\n",
      "\t\t\t<ymax>269</ymax>\r\n",
      "\t\t</bndbox>\r\n",
      "\t</object>\r\n",
      "</annotation>\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/train/apple_3.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdQxrA5_MpSg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Релизуйте выборку для YoloV1 - 2 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QXG9reop-BkS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "import tqdm\n",
    "import xmltodict\n",
    "\n",
    "from IPython.core.display import struct\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "# Добавьте необходимые вам библиотеки, если их не окажется в списке выше\n",
    "\n",
    "import xml.etree.ElementTree as ET #for xml parsing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gL8_CyyTYJ-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Так как в этом домашнем задании использовать аугментации для обучения __обязательно__ - советуем воспользоваться библиотекой albumentations.\n",
    "\n",
    "Она  особенно удобна, поскольку умеет сама вычислять новые координаты bounding box'ов после трансформаций картинки. Для знакомства с этим механизмом советуем следующий [гайд](hts://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/).\n",
    "\n",
    "Вы все еще можете избрать путь torchvision.transforms, вам потребуется знакомый нам метод `__getitem__`, однако вычислять новые координаты bounding box'ов после трансформаций вам придётся вручную\n",
    "\n",
    "__Обратите внимание__ на то, что в статье коробки предсказаний параметризуются через: _(x_center, y_center, width, height)_ (причем эти значения _относительные_), а в наших файлах - это _(x_min, y_min, x_max, y_max)_\n",
    "\n",
    "Также, помните что модель должна предсказывать как прямоугольник с обьектом, так и вероятности каждого класса!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def biection(data_dir):\n",
    "    \"\"\"\"\n",
    "    Сопоставляет каждой картинке и таблице в папке номер\n",
    "    -------------------------\n",
    "    return: словарь с ключами - названием картинки/таблицы, объектами - номерами\n",
    "    \"\"\"\n",
    "    w_path = os.path.join(os.getcwd(),data_dir)\n",
    "    answ = {}\n",
    "    count = 0\n",
    "    for filename in os.listdir(w_path):\n",
    "        name =  filename.split(\".\")[0]\n",
    "        if name not in answ.keys():\n",
    "            answ[name] = count\n",
    "            count += 1\n",
    "    return answ\n",
    "\n",
    "2 * len(biection(\"./data/test\")) == len(os.listdir(os.path.join(os.getcwd(),\"./data/test\")))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_paths(names_to_num, data_dir):\n",
    "    \"\"\"\"\n",
    "    Возвращает словарь путей до картинок и таблиц с коробками, причём\n",
    "    индексация в различных словарях совпадает\n",
    "    -------------------------\n",
    "    return: словарь с ключами - цифрами, объектами - ссылки\n",
    "    \"\"\"\n",
    "    w_path = os.path.join(os.getcwd(),data_dir)\n",
    "    i_path = {}\n",
    "    b_path = {}\n",
    "    for name in list(names_to_num.keys()):\n",
    "        num = names_to_num[name]\n",
    "        i_path[num] = os.path.join(w_path,(name + \".jpg\"))\n",
    "        b_path[num] = os.path.join(w_path,(name + \".xml\"))\n",
    "    return i_path, b_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "path_1 = '/Users/fedor/DataspellProjects/IDA_HW_3/data/test/orange_77.jpg'\n",
    "h = np.array(Image.open(path_1).convert(\"RGB\")).shape[0]\n",
    "w = np.array(Image.open(path_1).convert(\"RGB\")).shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tjZkU0vzMpSh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class2tag = {\"apple\": 1, \"orange\": 2, \"banana\": 3}\n",
    "\n",
    "class FruitDataset(Dataset):\n",
    "    def __init__(self, data_dir, transforms=None):\n",
    "        self.biection = biection(data_dir)\n",
    "        self.image_paths , self.box_paths = get_paths(self.biection, data_dir)\n",
    "        assert len(self.image_paths) == len(self.box_paths)\n",
    "        self.transforms = transforms\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array( Image.open(self.image_paths[idx]).convert(\"RGB\") )\n",
    "        bboxes, class_labels = self.__get_boxes_from_xml(self.box_paths[idx])\n",
    "        bboxes = (\n",
    "            self.__convert_to_yolo_box_params(\n",
    "                                        bboxes,\n",
    "                                        image.shape[1],\n",
    "                                        image.shape[0])\n",
    "                                                )\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image = image, bboxes = bboxes, class_labels = class_labels)\n",
    "            bboxes = transformed['bboxes']\n",
    "            image = transformed['image']\n",
    "            class_labels =  transformed['class_labels']\n",
    "\n",
    "        return torch.Tensor(image).permute(2, 0, 1), (bboxes, class_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __get_boxes_from_xml(self, xml_filename: str):\n",
    "        \"\"\"\n",
    "          Метод, который считает и распарсит (с помощью xmltodict) переданный xml\n",
    "          файл и вернет координаты прямоугольников объектов на соответсвующей фотографии\n",
    "          и название класса обьекта в каждом прямоугольнике\n",
    "        --------------------------\n",
    "        return:\n",
    "        boxes - format lists of lists [xmin, ymin, xmax, ymax]\n",
    "        name - list of\n",
    "        \"\"\"\n",
    "        root_node = ET.parse(xml_filename).getroot()\n",
    "        boxes = []\n",
    "        class_labels = []\n",
    "\n",
    "        for object in root_node.iter('object'):# Get the value from the attribute 'object'\n",
    "            name = object.find('name').text\n",
    "            xmin = int(object.find('bndbox').find('xmin').text)\n",
    "            ymin = int(object.find('bndbox').find('ymin').text)\n",
    "            xmax=  int(object.find('bndbox').find('xmax').text)\n",
    "            ymax = int(object.find('bndbox').find('ymax').text)\n",
    "\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            class_labels.append(class2tag[name])\n",
    "\n",
    "        return boxes, class_labels\n",
    "\n",
    "    def __convert_to_yolo_box_params(self, all_box_coordinates: List[int], im_w, im_h):\n",
    "        \"\"\"\n",
    "        Перейти от [xmin, ymin, xmax, ymax] к [x_center, y_center, width, height].\n",
    "\n",
    "        Обратите внимание, что параметры [x_center, y_center, width, height] - это\n",
    "        относительные значение в отрезке [0, 1]\n",
    "\n",
    "        :param: box_coordinates - координаты коробки в формате [xmin, ymin, xmax, ymax]\n",
    "        :param: im_w - ширина исходного изображения\n",
    "        :param: im_h - высота исходного изображения\n",
    "\n",
    "        :return: координаты коробки в формате [x_center, y_center, width, height]\n",
    "        \"\"\"\n",
    "        totall_anw = []\n",
    "        for box_coordinates in all_box_coordinates:\n",
    "            ans = []\n",
    "            ans.append((box_coordinates[0] + box_coordinates[2]) / 2 / im_w)  # x_center\n",
    "            ans.append((box_coordinates[1] + box_coordinates[3]) / 2 / im_h)  # y_center\n",
    "\n",
    "            ans.append((box_coordinates[2] - box_coordinates[0]) / im_w)  # width\n",
    "            ans.append((box_coordinates[3] - box_coordinates[1]) / im_h)  # height\n",
    "            totall_anw.append(ans)\n",
    "\n",
    "        return totall_anw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OwXeSiAjdGeq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "WIDTH, HEIGHT = 448, 448\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    albumentations.augmentations.geometric.resize.Resize(height=HEIGHT, width = WIDTH),\n",
    "    A.RandomRotate90(p=0.3),\n",
    "    A.HorizontalFlip(p=0.3),\n",
    "    A.Transpose(p=0.3),\n",
    "    A.PixelDropout(p=0.5),\n",
    "    A.Blur(blur_limit=(3,4), p=0.5)\n",
    "                            ],\n",
    "                             bbox_params=A.BboxParams(format='yolo',\n",
    "                                                      label_fields=['class_labels']))\n",
    "test_transform = A.Compose([\n",
    "    albumentations.augmentations.geometric.resize.Resize(height=HEIGHT, width = WIDTH)\n",
    "                            ],\n",
    "                            bbox_params=A.BboxParams(format='yolo',\n",
    "                                                     label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ayPwbRKocdCE",
    "outputId": "c7ec7134-c763-435c-a575-de2e16122a82",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты успешно пройдены\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FruitDataset(\n",
    "    transforms=train_transform,\n",
    "    data_dir=\"./data/train\"\n",
    "    )\n",
    "\n",
    "val_dataset = FruitDataset(\n",
    "    transforms=test_transform, \n",
    "    data_dir=\"./data/test\"\n",
    "    )\n",
    "\n",
    "# Немного проверок, чтобы убедиться в правильности направления решения\n",
    "assert isinstance(train_dataset[0], tuple)\n",
    "assert len(train_dataset[0]) == 2\n",
    "assert isinstance(train_dataset[0][0], torch.Tensor)\n",
    "assert isinstance(train_dataset[0][1], tuple)\n",
    "assert len(train_dataset[0][1]) == 2\n",
    "print(\"Тесты успешно пройдены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9V1Tl_GAdeIv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size= 1,\n",
    "    shuffle=True)\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fRR9ns6MpSh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Теперь определим функцию для рассчета Intersection Over Union по 4 углам двух прямоугольников"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Rd88hnZiMpSh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(predicted_bbox, gt_bbox) -> float:\n",
    "    \"\"\"\n",
    "    Intersection Over Union для двух прямоугольников\n",
    "\n",
    "    :param: dt_bbox - [x_min, y_min, x_max, y_max]\n",
    "    :param: gt_bbox - [x_min, y_min, x_max, y_max]\n",
    "\n",
    "    :return: Intersection Over Union\n",
    "    \"\"\"\n",
    "\n",
    "    # intersection_bbox = np.array(\n",
    "    intersection_bbox = torch.Tensor(\n",
    "        [\n",
    "            max(predicted_bbox[0], gt_bbox[0]),\n",
    "            max(predicted_bbox[1], gt_bbox[1]),\n",
    "            min(predicted_bbox[2], gt_bbox[2]),\n",
    "            min(predicted_bbox[3], gt_bbox[3]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    intersection_area = max(intersection_bbox[2] - intersection_bbox[0], 0) \\\n",
    "                        * max( intersection_bbox[3] - intersection_bbox[1], 0)\n",
    "    area_dt = (predicted_bbox[2] - predicted_bbox[0]) * (predicted_bbox[3] - predicted_bbox[1])\n",
    "    area_gt = (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1])\n",
    "\n",
    "    union_area = area_dt + area_gt - intersection_area\n",
    "\n",
    "    iou = intersection_area / union_area\n",
    "    return float(iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVJWo3xbMpSh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Теперь начинается основная часть домашнего задания: обучите модель YOLO для object detection на __обучающем__ датасете. \n",
    "\n",
    " - Создайте модель и функцию ошибки YoloV1 прочитав [оригинальную статью](https://paperswithcode.com/paper/you-only-look-once-unified-real-time-object)\n",
    " - Напишите функцию обучения модели\n",
    " - Используйте аугментации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxfMVwzHW2MJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Реализуйте Модель - 2 балла\n",
    "\n",
    "Копировать точное количество слоев и параметры сверток необязательно. Главное - чтобы модель работала по принципу, описанному в статье и делала предсказание в представленном формате.\n",
    "\n",
    "\n",
    "В качестве подсказки напомним, что выходом модели __для каждого обьекта__ должен быть тензор размера\n",
    "__S * S * (B * 5 + С)__, где все параметры имеют такое же значение, как и в статье: \n",
    "\n",
    "- S - количество ячеек на которое разбивается изображение по вертикали/горизонтали\n",
    "- В - количество предсказываемых прямоугольников в каждой ячейке\n",
    "- 5 - количество параметров для определения каждого прямоугольника (x_center, y_center, width, height, confidence)\n",
    "- С - количество классов (apple, banana, orange)\n",
    "\n",
    "Таким образом, мы для каждого окна размера __S x S__ предсказываем __В__ коробо и один класс"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):  # можно поменять на Lightning\n",
    "    def __init__(self, convs,out_channels, is_max_pool:bool=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv_layers = convs\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "        self.is_maxpool = is_max_pool  # не после каждой свертки нужно делать maxpool\n",
    "        self.maxpool = nn.MaxPool2d(2,stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        if len(x.size()) == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.leakyrelu(self.batchnorm(x))\n",
    "\n",
    "        if self.is_maxpool:\n",
    "            x = self.maxpool(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# conv_layer_1 = nn.Sequential(\n",
    "#     nn.Conv2d(in_channels=3,  out_channels = 192, kernel_size=3, stride =2, padding=1))\n",
    "#\n",
    "# conv_layer_2 = nn.Sequential(\n",
    "#     nn.Conv2d(in_channels=192, out_channels=256, kernel_size=3, padding=1))\n",
    "#\n",
    "# conv_layer_3 = nn.Sequential(\n",
    "#     nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n",
    "#     nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "#     nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1),\n",
    "#     nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "# )\n",
    "#\n",
    "# conv_layer_4 = nn.Sequential(\n",
    "#     nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),#1\n",
    "#     nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "#     nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),#2\n",
    "#     nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "#     nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),#3\n",
    "#     nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "#     nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),#4\n",
    "#     nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "#\n",
    "#     nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1),\n",
    "#     nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1)\n",
    "# )\n",
    "#\n",
    "# conv_layer_5 = nn.Sequential(\n",
    "#     nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1),#1\n",
    "#     nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1),\n",
    "#     nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1),#2\n",
    "#     nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1),\n",
    "#\n",
    "#     nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1),\n",
    "#     nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3,stride = 2, padding=1)\n",
    "# )\n",
    "#\n",
    "# conv_layer_6 = nn.Sequential(\n",
    "#     nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1),\n",
    "#     nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1)\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "conv_layer_1 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3,  out_channels = 192, kernel_size=3, stride =2, padding=1))\n",
    "\n",
    "conv_layer_2 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=192, out_channels=256, kernel_size=3, padding=1))\n",
    "\n",
    "conv_layer_3 = nn.Sequential(\n",
    "    # nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n",
    "    # nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "    nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1),\n",
    "    nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    ")\n",
    "\n",
    "conv_layer_4 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),#1\n",
    "    nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "    nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),#2\n",
    "    nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "    # nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),#3\n",
    "    # nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "    # nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),#4\n",
    "    # nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "\n",
    "    nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1),\n",
    "    nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1)\n",
    ")\n",
    "\n",
    "conv_layer_5 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1),#1\n",
    "    nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1),\n",
    "    # nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1),#2\n",
    "    # nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1),\n",
    "\n",
    "    # nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1),\n",
    "    nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3,stride = 2, padding=1)\n",
    ")\n",
    "\n",
    "conv_layer_6 = nn.Sequential(\n",
    "    # nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1),\n",
    "    nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, padding=1)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "tensor([[[-8.9236e-02,  3.7567e-01,  6.2909e-02, -3.0973e-01, -9.4400e-02,\n",
      "           1.5599e-02,  2.3924e-02,  5.7563e-02,  2.8630e-01,  6.9651e-02,\n",
      "           7.9393e-02, -1.2314e-01, -8.1455e-02],\n",
      "         [ 2.1940e-01,  1.8086e-01, -3.3215e-01, -2.6038e-01,  1.3904e-01,\n",
      "          -4.6683e-02,  1.5579e-02, -5.3946e-02,  6.8055e-02,  3.4724e-01,\n",
      "          -8.0993e-02,  7.2766e-02, -1.9864e-01],\n",
      "         [ 5.9243e-03, -2.2191e-03,  5.0802e-02,  2.8722e-01,  2.8957e-01,\n",
      "           1.4581e-01, -4.0938e-02, -6.8013e-02, -1.4862e-02,  2.5626e-01,\n",
      "           2.8764e-01,  9.6467e-03,  3.2625e-02],\n",
      "         [ 6.5186e-02,  2.7643e-01, -1.8231e-01, -8.7535e-03, -1.4182e-01,\n",
      "           5.1804e-02, -4.9984e-02, -6.1981e-02,  4.7091e-01, -3.7214e-01,\n",
      "           1.7597e-01, -9.9572e-02, -2.3385e-01],\n",
      "         [-1.8345e-01, -1.8877e-01, -5.1851e-01,  8.0753e-02,  9.6471e-03,\n",
      "          -1.4020e-01,  2.6317e-01,  1.7989e-01, -5.8542e-02,  3.7141e-01,\n",
      "          -2.8631e-02, -3.1901e-01, -6.6893e-02],\n",
      "         [ 6.3715e-02,  4.5503e-01, -2.3126e-04,  1.9142e-03,  6.1795e-02,\n",
      "           1.8521e-01,  1.1925e-01, -7.4347e-03, -1.0760e-01,  4.2803e-02,\n",
      "           2.8515e-01,  1.6712e-01, -2.0557e-01],\n",
      "         [-1.2014e-01,  1.2779e-01, -2.6331e-01,  1.0338e-01, -1.0006e-01,\n",
      "          -5.1434e-02,  1.1565e-01, -8.8842e-02,  4.2434e-01,  1.3078e-01,\n",
      "          -1.7196e-02,  1.9097e-01,  1.6057e-01]],\n",
      "\n",
      "        [[-7.4423e-02, -1.0229e-01, -1.1719e-01, -1.2534e-01, -2.0471e-01,\n",
      "           2.6021e-03,  2.0983e-01,  1.1504e-01, -1.0558e-01,  1.5513e-01,\n",
      "           9.3437e-02, -1.8549e-01,  1.4017e-01],\n",
      "         [ 9.3286e-02, -1.3036e-01, -3.3595e-01, -2.1658e-01, -1.1174e-01,\n",
      "           2.6933e-01, -1.1593e-01, -1.3056e-01,  1.6784e-01,  2.2436e-02,\n",
      "          -1.0514e-01,  1.0403e-01,  3.1541e-01],\n",
      "         [ 4.2593e-01,  1.6698e-01,  7.1931e-03,  2.4154e-01, -1.0768e-02,\n",
      "           2.9820e-01,  3.9094e-01,  1.6457e-01,  9.3644e-02, -1.1820e-02,\n",
      "          -9.0912e-02, -1.4884e-02, -1.1020e-01],\n",
      "         [-1.3312e-01, -5.2221e-02,  1.4004e-01, -1.7966e-01, -2.8040e-01,\n",
      "          -1.6920e-01,  7.4899e-02, -3.2007e-01, -3.2286e-01, -8.0310e-02,\n",
      "          -1.6860e-02, -1.7109e-02,  3.3846e-01],\n",
      "         [-3.4781e-01, -1.4655e-02, -1.8393e-01, -1.7528e-01,  9.8243e-02,\n",
      "          -2.6288e-01,  1.6754e-01,  1.0312e-01, -4.4331e-02,  1.4368e-01,\n",
      "           8.1409e-02,  6.6121e-02,  2.4019e-01],\n",
      "         [ 7.0765e-02,  4.0770e-02, -5.9049e-02,  1.8703e-01,  8.8516e-02,\n",
      "          -6.0087e-02,  1.6510e-01, -6.5821e-02,  9.1072e-03, -9.3361e-02,\n",
      "          -5.5317e-02,  3.0899e-02, -2.1300e-01],\n",
      "         [-2.1059e-02,  1.3623e-02, -1.4100e-01,  4.6429e-02, -2.0955e-01,\n",
      "          -6.7587e-02, -2.8195e-01,  5.1460e-03,  9.2382e-02,  1.0424e-01,\n",
      "          -2.4954e-01, -4.4029e-01, -1.3344e-01]],\n",
      "\n",
      "        [[-1.8736e-03,  3.2772e-01,  3.7720e-04,  1.9161e-02, -1.9863e-01,\n",
      "           2.7504e-02,  1.6983e-02,  2.2976e-01, -2.1577e-01, -2.0522e-01,\n",
      "           2.0764e-02,  9.7715e-02, -7.3152e-02],\n",
      "         [ 2.5355e-01,  1.0552e-02, -5.3791e-02,  4.1993e-01, -3.3801e-01,\n",
      "           1.7829e-01, -7.8865e-03,  1.7642e-01,  5.3351e-02,  7.5477e-04,\n",
      "          -9.1447e-04, -3.9838e-02,  1.4989e-01],\n",
      "         [ 3.1328e-02,  3.7682e-01, -2.6857e-01,  7.9302e-02, -1.2470e-01,\n",
      "          -7.9275e-02, -2.3927e-01, -1.6266e-01, -2.5087e-01, -1.4208e-01,\n",
      "           1.6447e-01, -1.1508e-01, -4.0078e-01],\n",
      "         [-1.9911e-02, -3.7839e-01,  1.5814e-01, -4.8482e-02,  1.2397e-01,\n",
      "           5.9975e-02, -2.3270e-01,  6.1640e-02,  1.4034e-01,  2.0411e-02,\n",
      "          -2.3993e-01, -1.1624e-01,  1.3132e-01],\n",
      "         [ 9.6396e-02,  1.7987e-01,  2.2001e-01, -8.0252e-02, -7.1445e-02,\n",
      "          -1.4765e-01,  3.8088e-02, -1.1772e-01, -3.5359e-02,  3.9004e-01,\n",
      "           1.8811e-01,  8.5567e-03, -1.1109e-01],\n",
      "         [ 2.2790e-02, -2.7380e-01, -1.0609e-01, -2.8167e-01,  1.6028e-01,\n",
      "          -2.1013e-01,  1.1126e-01, -2.9039e-02,  1.4335e-01,  2.2481e-02,\n",
      "          -1.3054e-01, -2.0262e-01,  1.0608e-01],\n",
      "         [ 1.0496e-01, -7.9940e-04,  1.1517e-01, -6.1555e-02, -2.1100e-01,\n",
      "          -6.2607e-02,  2.3278e-01, -3.5287e-01,  1.2268e-01,  1.6862e-01,\n",
      "          -5.7803e-03,  1.0169e-01, -1.0646e-01]],\n",
      "\n",
      "        [[-1.4307e-01, -1.8173e-01, -4.8015e-02,  6.5297e-02,  1.6437e-02,\n",
      "           1.7877e-01,  1.5343e-01,  3.1708e-01, -2.4802e-01,  1.0173e-01,\n",
      "           8.3155e-02, -2.5030e-01, -4.2077e-02],\n",
      "         [ 7.6027e-02,  1.0283e-01, -2.0817e-01, -1.9829e-01, -1.4243e-01,\n",
      "           3.3202e-01,  8.1342e-02, -1.3459e-01,  8.8980e-02,  1.5791e-02,\n",
      "          -6.5034e-02, -1.1995e-02,  2.7176e-01],\n",
      "         [-2.3025e-02, -1.9111e-01, -9.2205e-02,  1.4372e-01, -2.0235e-01,\n",
      "          -5.8439e-02,  1.2155e-01, -9.0947e-02, -2.0889e-01, -1.0505e-01,\n",
      "           5.2397e-01,  5.9530e-02, -2.9904e-02],\n",
      "         [-2.2420e-01, -1.2008e-01, -4.6545e-02,  2.8137e-01, -2.6835e-02,\n",
      "          -8.6074e-02, -7.1848e-02,  2.1488e-01,  3.6788e-02, -9.6417e-02,\n",
      "          -6.1565e-02, -7.4265e-02,  1.2207e-01],\n",
      "         [ 3.2950e-02, -7.6856e-02, -3.8318e-02, -2.1828e-01, -7.7348e-02,\n",
      "           1.8529e-01,  3.0294e-01, -9.1143e-02,  2.0885e-01, -1.0395e-01,\n",
      "          -3.3055e-02,  1.6230e-01, -4.3355e-01],\n",
      "         [ 1.9245e-01, -7.1826e-02, -2.0478e-01,  1.2207e-01, -3.0697e-02,\n",
      "           1.7618e-01, -3.1795e-01, -2.4496e-01,  1.3683e-01,  1.6260e-01,\n",
      "           1.0222e-01,  6.3673e-02, -7.0508e-02],\n",
      "         [-2.4943e-01,  3.8613e-01, -3.8015e-02,  8.1598e-02, -4.5539e-02,\n",
      "          -1.5335e-01, -1.5731e-02,  1.1868e-01,  1.6497e-01, -1.9067e-02,\n",
      "           1.0926e-01, -1.2317e-01,  1.2679e-01]],\n",
      "\n",
      "        [[ 6.7840e-02,  7.7623e-02,  1.2833e-01, -5.8804e-02,  1.1915e-02,\n",
      "           3.1080e-02,  2.5352e-02,  1.3932e-01, -5.0712e-02,  1.6181e-01,\n",
      "          -1.6190e-01, -3.9728e-01,  5.0810e-01],\n",
      "         [ 3.8210e-02, -2.1713e-01,  1.1592e-01,  7.2674e-03,  1.5803e-01,\n",
      "          -1.1019e-01,  6.7012e-02,  1.7375e-02,  1.7163e-01, -9.5985e-02,\n",
      "           6.5105e-02, -1.3437e-01,  3.3093e-01],\n",
      "         [-1.9771e-01, -6.4857e-02,  1.1024e-01, -1.2378e-01, -1.5022e-02,\n",
      "          -2.2578e-01, -3.0446e-02, -2.1094e-01,  2.3014e-01,  2.9320e-01,\n",
      "          -2.3049e-01, -7.8131e-02, -1.8428e-01],\n",
      "         [ 2.6950e-02,  1.3264e-01,  1.8078e-01, -2.0853e-01, -4.5276e-02,\n",
      "          -3.8431e-01,  2.3434e-02, -1.3882e-01, -1.6885e-01, -2.2839e-01,\n",
      "           1.9186e-01,  2.0576e-02, -3.3377e-01],\n",
      "         [ 1.9499e-01, -1.4202e-01, -1.3949e-01,  4.3871e-01,  1.5484e-03,\n",
      "          -2.7777e-01, -6.0929e-02,  1.4398e-01,  3.6145e-01,  1.0975e-01,\n",
      "          -2.8898e-01, -2.2382e-01,  1.3676e-01],\n",
      "         [-9.5387e-02,  1.7994e-01,  1.2743e-01, -3.8172e-02, -1.4089e-01,\n",
      "           2.1030e-01, -2.4415e-01, -2.5375e-02, -1.3556e-01, -1.1985e-01,\n",
      "           1.6415e-01,  1.5421e-01, -4.8892e-02],\n",
      "         [-4.6011e-02,  1.7007e-01, -2.3244e-01,  5.2137e-03,  3.7551e-01,\n",
      "           2.8569e-02, -2.2202e-01, -1.9224e-01, -3.6696e-01, -1.5981e-01,\n",
      "           7.4286e-04,  1.5875e-01,  6.1440e-02]],\n",
      "\n",
      "        [[ 8.6540e-02,  2.8592e-02,  2.2864e-01,  3.0998e-02, -8.9652e-02,\n",
      "          -6.9589e-02,  3.2763e-01, -1.0715e-01,  2.3965e-01,  2.4719e-01,\n",
      "           1.2179e-01, -8.3293e-02,  3.3704e-01],\n",
      "         [ 7.1847e-02, -1.9440e-01,  2.0182e-01,  8.3873e-02,  6.3147e-02,\n",
      "           1.9564e-01,  3.9749e-02,  2.7391e-01, -8.5119e-02,  1.2542e-01,\n",
      "          -1.3028e-01, -1.7586e-01, -1.3760e-01],\n",
      "         [ 2.8167e-01, -1.5218e-01,  5.0754e-02,  1.0585e-01,  1.7093e-01,\n",
      "           3.4867e-01,  1.7394e-01, -3.6998e-02,  2.6100e-01,  2.9881e-02,\n",
      "           2.6712e-01, -3.3547e-01,  5.7254e-02],\n",
      "         [-7.0459e-02,  2.0580e-01,  3.9719e-02,  1.1552e-01,  4.9969e-02,\n",
      "          -1.9574e-01, -1.3073e-01, -6.1215e-02,  1.1172e-01,  2.3099e-01,\n",
      "           9.1922e-02,  2.4622e-01, -1.7853e-01],\n",
      "         [-1.5578e-01,  7.1816e-02, -6.3856e-02,  5.0189e-02,  1.4131e-01,\n",
      "          -5.1561e-02,  1.4870e-01,  1.5998e-01,  1.4439e-01,  8.8268e-02,\n",
      "           8.0987e-03, -5.1424e-02,  1.6876e-01],\n",
      "         [ 2.1698e-02,  4.9520e-02,  1.1833e-01,  5.4557e-02,  1.7727e-01,\n",
      "          -1.8236e-02,  2.8527e-02, -6.6288e-03,  1.3312e-01, -7.3311e-02,\n",
      "          -2.8357e-01,  1.1529e-01,  1.2714e-01],\n",
      "         [-5.1488e-01,  5.4078e-02, -6.1727e-03, -7.2692e-02,  2.0746e-01,\n",
      "           1.8456e-01, -2.3045e-01, -1.1822e-01, -5.1663e-02, -4.3011e-02,\n",
      "           1.0488e-01,  3.2918e-01,  3.2477e-02]],\n",
      "\n",
      "        [[-1.5533e-01, -1.1760e-02, -1.1924e-02, -1.0110e-01, -1.8510e-01,\n",
      "          -6.8493e-02, -1.8033e-01, -1.4195e-01, -2.7354e-01, -8.6576e-02,\n",
      "          -1.4129e-02, -6.9171e-02, -2.8197e-02],\n",
      "         [-2.0660e-01,  3.3911e-01, -2.4231e-01,  2.1752e-01,  1.6879e-01,\n",
      "          -2.2047e-02,  1.9549e-01,  1.3216e-03, -4.4916e-02,  2.4371e-02,\n",
      "          -4.4660e-01, -1.2372e-01,  6.4773e-02],\n",
      "         [-1.6263e-02,  2.7810e-01, -2.1481e-01,  4.3812e-01,  2.9394e-01,\n",
      "          -2.1005e-01,  8.9900e-02,  1.5399e-01, -3.0362e-02,  2.4649e-01,\n",
      "           2.1871e-02,  9.0782e-03,  4.2971e-01],\n",
      "         [-1.9286e-01, -1.9406e-02,  5.2377e-02,  2.9489e-02,  1.3631e-01,\n",
      "          -1.5436e-01, -3.3007e-01, -2.7265e-01,  7.2532e-02, -5.1220e-02,\n",
      "           2.4223e-01,  5.4161e-02,  2.1033e-01],\n",
      "         [-5.1265e-02,  2.4562e-01, -7.3327e-02,  1.6868e-01,  9.3129e-02,\n",
      "          -1.3878e-01, -3.2244e-02,  2.2294e-02,  2.6995e-01, -6.1694e-02,\n",
      "          -1.0073e-01,  1.9360e-01, -6.4181e-02],\n",
      "         [ 1.7285e-02, -1.5237e-01,  9.1982e-02, -2.0653e-01,  2.1801e-01,\n",
      "           1.7250e-01,  4.1103e-01,  8.3634e-02,  2.2068e-01,  2.4814e-04,\n",
      "          -3.6953e-01, -8.9243e-02,  1.0097e-01],\n",
      "         [-6.7231e-02,  2.1061e-01,  9.5602e-02, -1.6242e-02, -5.6486e-02,\n",
      "           1.2397e-01, -4.4565e-02,  2.2894e-01,  9.6364e-02,  1.5552e-01,\n",
      "           2.5902e-01,  1.3880e-01, -2.3246e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# tests\n",
    "a  = CNNBlock(conv_layer_1, 192, True)\n",
    "x = a.forward(train_dataset.__getitem__(idx = 0)[0])\n",
    "print(x.size() == torch.Size([1, 192, 112, 112]))\n",
    "b = CNNBlock(conv_layer_2, 256, True)\n",
    "x = b.forward(x)\n",
    "print(x.size() == torch.Size([1, 256, 56, 56]))\n",
    "c = CNNBlock(conv_layer_3, 512, True)\n",
    "x = c.forward(x)\n",
    "print(x.size() == torch.Size([1, 512, 28, 28]))\n",
    "d = CNNBlock(conv_layer_4, 1024, True)\n",
    "x = d.forward(x)\n",
    "print(x.size() == torch.Size([1, 1024, 14, 14]))\n",
    "e = CNNBlock(conv_layer_5, 1024, False)\n",
    "x = e.forward(x)\n",
    "print(x.size() == torch.Size([1, 1024, 7, 7]))\n",
    "\n",
    "f = CNNBlock(conv_layer_6, 512, False) #1024\n",
    "x = f.forward(x)\n",
    "\n",
    "print(x.size() == torch.Size([1, 1024, 7, 7]))\n",
    "l1 = nn.Linear(7*7*512,4096)\n",
    "out = nn.Flatten()(x)\n",
    "out = l1(out)\n",
    "out = nn.LeakyReLU(0.1)(out)\n",
    "S=7\n",
    "B=2\n",
    "C=3\n",
    "l2 = nn.Linear(4096,S*S*(B*5 + C))\n",
    "final = l2(out).view(S,S,B*5 + C)\n",
    "print(final)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3PJwrvcWW1n7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class YOLO(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=3):\n",
    "        \"\"\"\n",
    "        :param: S * S - количество ячеек на которые разбивается изображение\n",
    "        :param: B - количество предсказанных прямоугольников в каждой ячейке\n",
    "        :param: C - количество классов\n",
    "        \"\"\"\n",
    "        super(YOLO, self).__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.conv_block_1 = CNNBlock(conv_layer_1, 192, True)\n",
    "        self.conv_block_2 = CNNBlock(conv_layer_2, 256, True)\n",
    "        self.conv_block_3 = CNNBlock(conv_layer_3, 512, True)\n",
    "        self.conv_block_4 = CNNBlock(conv_layer_4, 1024, True)\n",
    "        self.conv_block_5 = CNNBlock(conv_layer_5, 1024, False)\n",
    "        self.conv_block_6 = CNNBlock(conv_layer_6, 512, False) #1024\n",
    "        self.linear_1 =  nn.Linear(7*7*512,1024) # 7 7 1024, 4096\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "        self.linear_2 =  nn.Linear(1024,S*S*(B*5 + C)) #2048,S*S*(B*5 + C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.conv_block_3(x)\n",
    "        x = self.conv_block_4(x)\n",
    "        x = self.conv_block_5(x)\n",
    "        x = self.conv_block_6(x)\n",
    "        x = self.leaky_relu(\n",
    "            self.linear_1(\n",
    "                nn.Flatten()(x)\n",
    "            )\n",
    "        )\n",
    "        x = self.linear_2(x)\n",
    "        x= x.view(self.S,self.S,self.B*5 + self.C)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "temp_model = YOLO()\n",
    "expected_output_shape = temp_model.S * temp_model.S * (5 * temp_model.B + temp_model.C)\n",
    "\n",
    "assert temp_model(train_dataset[0][0]).reshape(-1).shape[0] == expected_output_shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# model_yolo = YOLO()\n",
    "# pred = model_yolo.forward(train_dataset.__getitem__(idx=1)[0])\n",
    "# pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJIjWKbcYUYe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Реализуйте YoloLoss - 3 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# S × S × (B ∗ 5 + C) tensor     x, y, w, h, and confidence\n",
    "# “responsible” for predicting an object based on which\n",
    "# prediction has the highest current IOU with the ground\n",
    "# truth\n",
    "# x, y, w, h, c,    x, y, w, h, c,   cl, cl, cl ---- pred\n",
    "# x, y, w, h,    cl, cl, cl --- target\n",
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=3):\n",
    "        \"\"\"\n",
    "        :param: S * S - количество ячеек на которые разбивается изображение\n",
    "        :param: B - количество предсказанных прямоугольников в каждой ячейке\n",
    "        :param: C - количество классов\n",
    "        :param: lambda_noobj - константа для обозначения важности ячеек без объектов\n",
    "        :param: objects_dict - константа для обозначения важности ячеек с объектами\n",
    "        :param: objects_dict - словарь\n",
    "         с ключами -  координатами (i,j) ячеек содержащих объекты\n",
    "         с значениями - таргет значениями для этих ячеек в формате [(b_box, class)]\n",
    "        :param: class2vec - словарь\n",
    "         с ключами числовыми метками классов\n",
    "         с значениями one-hot векторами этого класса\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "        self.objects_dict = {}\n",
    "        self.class2vec = {1:[1,0,0],2:[0,1,0],3:[0,0,1]}\n",
    "\n",
    "    def _get_best_prediction(self, predictions,target):\n",
    "        \"\"\"\n",
    "        для всех ячеек в матрице S*S находит \"ответственные\" значения среди В вариантов\n",
    "        :param predictions: - предсказания модели YOLO\n",
    "        :param target:  - целевые значения\n",
    "        :return best_box_mask: torch.Tensor([S,S,B*5+C])best_box_mask - бинарная маска\n",
    "        \"\"\"\n",
    "        predictions = predictions.squeeze(0)\n",
    "        best_box_mask = torch.zeros([self.S,self.S,self.B*5+self.C])\n",
    "\n",
    "        for i in range(self.S):\n",
    "            for j in range(self.S):\n",
    "                best_box_mask[i,j,-self.C:] = 1\n",
    "                if (i,j) not in self.objects_dict.keys():\n",
    "                    best_box_mask[i,j,[4,9]] = 1 # понадобиться когда мы будем уменьшать уверенность для ячеек бкз объекта\n",
    "                    continue\n",
    "                max_iou=0\n",
    "                best_1, best_2 = (0,5)\n",
    "                for k in range(0,5*self.B,5):\n",
    "\n",
    "                    if intersection_over_union(predictions[i,j,k:k+4], self.objects_dict[(i,j)][0][0]) >= max_iou:\n",
    "                        best_1, best_2 = (k,k+5)\n",
    "                        max_iou = intersection_over_union(predictions[i,j,k:k+4], self.objects_dict[(i,j)][0][0])\n",
    "\n",
    "                best_box_mask[i,j,best_1: best_2] = 1\n",
    "\n",
    "        return best_box_mask\n",
    "\n",
    "    def _dict_init(self, target):\n",
    "        \"\"\"\n",
    "        инициализирует словарь objects_dict при помощи значений из target\n",
    "        :param target: - таргет значение\n",
    "        :return: nan\n",
    "        \"\"\"\n",
    "        self.objects_dict = {}\n",
    "        b_target = target[:-1][0]\n",
    "        clfsses = target[-1]\n",
    "        for b_box, clas in zip(b_target, clfsses):\n",
    "            c  = self._get_cel_cord(b_box)\n",
    "            if c not in self.objects_dict.keys():\n",
    "                self.objects_dict[c] = [(b_box,clas)]\n",
    "            else:\n",
    "                self.objects_dict[c] = self.objects_dict[c].append((b_box,clas))\n",
    "        return\n",
    "\n",
    "    def _get_cel_cord(self, bbox):\n",
    "        \"\"\"\n",
    "        по bounding box возвращает номер ячейки куда приходиться центр\n",
    "        :param bbox: окаймляющая рамка (bounding box) целевого значения\n",
    "        :return x, y: int, int - координаты ячейки (её номера)\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        x = self.S - 1\n",
    "        y = self.S-1\n",
    "        for cord in np.linspace(0,1,self.S+1)[:-1]:\n",
    "            if cord > bbox[0]:\n",
    "                x = count\n",
    "                break\n",
    "            count += 1\n",
    "\n",
    "        count = 0\n",
    "        for cord in np.linspace(0,1,self.S+1)[:-1]:\n",
    "            if cord > bbox[1]:\n",
    "                y = count\n",
    "                break\n",
    "            count += 1\n",
    "        return x, y\n",
    "\n",
    "    def _get_masks(self,target):\n",
    "        \"\"\"\n",
    "         Возращает бинарные маски, указывающие на то, есть ли объект в ячейке.\n",
    "         :param target: целевые значения\n",
    "         :return mask_obj: torch.Tensor([S,S,B*5+C]) - маска присутствия центра объекта\n",
    "         :return mask_noobj: torch.Tensor([S,S,B*5+C]) - маска отсутствия центра объекта\n",
    "        \"\"\"\n",
    "        mask_obj = torch.zeros([self.S,self.S,self.B*5+self.C])\n",
    "        mask_noobj = torch.ones([self.S,self.S,self.B*5+self.C])\n",
    "        for i,j in self.objects_dict.keys():\n",
    "            mask_obj[i,j] = 1\n",
    "            mask_noobj[i,j] = 0\n",
    "\n",
    "        return mask_obj,mask_noobj\n",
    "\n",
    "    def _set_target(self):\n",
    "        \"\"\"\n",
    "        возвращает target значение в формате тензора [S,S,5+C]\n",
    "        :return new_target:  - torch.Tensor([S,S,5+C]) - target тензор в формате\n",
    "        \"\"\"\n",
    "        new_target = torch.zeros([self.S,self.S,4+self.C])\n",
    "        for i,j in self.objects_dict.keys():\n",
    "            l = list(self.objects_dict[(i,j)][0][0])\n",
    "            self._get_YOLO_coordinates(i,j,l)\n",
    "            vec = self.class2vec[self.objects_dict[(i,j)][0][1][0].item()]\n",
    "            l = l+vec\n",
    "            new_target[i,j,:] = torch.Tensor(l)\n",
    "\n",
    "        return new_target\n",
    "\n",
    "    def _get_YOLO_coordinates(self,i,j,b_box):\n",
    "        \"\"\"\n",
    "        :param i: int - first coordinate of the cell\n",
    "        :param j: int - second coordinate of the cell\n",
    "        :param b_box: ittarable - bounding box parameters\n",
    "        :return: bounding box parameters in YOLO format\n",
    "        \"\"\"\n",
    "        b_box[0] = b_box[0] - (i*(1/(self.S)) + (1/(self.S))/2) #relative to center of cell i,j\n",
    "        b_box[1] = b_box[1] - (j*(1/(self.S)) + (1/(self.S))/2)\n",
    "        return b_box\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        \"\"\"\n",
    "        Считает loss значение для входных предсказаний и целевых значений\n",
    "        :param predictions: torch.Tensor([S,S,B*5+C]) - предсказания\n",
    "        :param target: (b_boxes, classes) - целевые значения\n",
    "        :return answ: torch.Tensor([1]) - значение loss\n",
    "        \"\"\"\n",
    "        self._dict_init(target)\n",
    "        mask_obj, mask_noobj = self._get_masks(target)\n",
    "        predictions = predictions.reshape(-1, self.S, self.S,  self.B * 5 + self.C)\n",
    "        target = self._set_target()\n",
    "        best_box = self._get_best_prediction(predictions, target)\n",
    "\n",
    "        # print(mask_obj)\n",
    "        # print(mask_noobj)\n",
    "        # print(predictions)\n",
    "        # print(\"target\",target)\n",
    "        # print(\"best_box\", best_box)\n",
    "        # print(mask_obj*best_box*predictions)\n",
    "        # X\n",
    "        answ = self.lambda_coord * self.mse((mask_obj*best_box*predictions)[...,[0]], target[...,0]) + \\\n",
    "               self.lambda_coord * self.mse((mask_obj*best_box*predictions)[...,[5]], target[...,0])\n",
    "\n",
    "        # print(\"x\",answ)\n",
    "        # Y\n",
    "        answ += self.lambda_coord * self.mse((mask_obj*best_box*predictions)[...,[1]], target[...,1]) + \\\n",
    "               self.lambda_coord * self.mse((mask_obj*best_box*predictions)[...,[6]], target[...,1])\n",
    "\n",
    "        # print(\"y\",answ)\n",
    "        # W\n",
    "        answ += self.lambda_coord * self.mse(\n",
    "            torch.sign((mask_obj*best_box*predictions)[...,[2]]) * torch.sqrt(\n",
    "                torch.abs((mask_obj*best_box*predictions)[...,[2]])+1e-03),\n",
    "            torch.sqrt(target[...,2]))+\\\n",
    "            self.lambda_coord * self.mse(\n",
    "            torch.sign((mask_obj*best_box*predictions)[...,[7]]) *torch.sqrt(\n",
    "                torch.abs((mask_obj*best_box*predictions)[...,[7]])+1e-03),\n",
    "            torch.sqrt(target[...,2]))\n",
    "\n",
    "        # print(\"w\",answ)\n",
    "        # H\n",
    "        answ += self.lambda_coord * self.mse(\n",
    "            torch.sign((mask_obj*best_box*predictions)[...,[3]]) * torch.sqrt(\n",
    "                torch.abs((mask_obj*best_box*predictions)[...,[3]])+1e-03),\n",
    "            torch.sqrt( target[...,3]) )+\\\n",
    "            self.lambda_coord * self.mse(\n",
    "            torch.sign((mask_obj*best_box*predictions)[...,[8]]) *torch.sqrt(\n",
    "                torch.abs((mask_obj*best_box*predictions)[...,[8]])+1e-03),\n",
    "            torch.sqrt( target[...,3]))\n",
    "\n",
    "        # print(\"h\",answ)\n",
    "        # C mask_obj\n",
    "        answ += self.mse((mask_obj*best_box*predictions)[...,[4]], best_box[...,[4]]*mask_obj[...,[4]]*torch.ones([self.S,self.S])) + \\\n",
    "               self.mse((mask_obj*best_box*predictions)[...,[9]], best_box[...,[4]]*mask_obj[...,[4]]*torch.ones([self.S,self.S]))\n",
    "\n",
    "        # print(\"c_obj\",answ)\n",
    "        # C mask_noobj\n",
    "        answ += self.lambda_noobj * self.mse((mask_noobj*best_box*predictions)[...,[4]], torch.zeros([self.S,self.S])) + \\\n",
    "               self.lambda_noobj * self.mse((mask_noobj*best_box*predictions)[...,[9]], torch.zeros([self.S,self.S]))\n",
    "\n",
    "        # print(\"c__nooj\",answ)\n",
    "        # Classes\n",
    "        answ += self.mse((mask_obj*predictions)[...,[10,11,12]],\n",
    "                                                target[...,[4,5,6]])\n",
    "        return answ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Обучение...\n",
    "Я знаю, что не успел нормально дообучить! Времени не хватило от слова совсем... Но по тем лоссам которые приведены у меня, видно что лосс-то падает! Я знаю, что на меня разгневаються боги ML-я, за то что я не построил граффики, но построить их правда не успел.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [],
   "source": [
    "y_pred = torch.Tensor()\n",
    "f_loss = YoloLoss()\n",
    "y = torch.Tensor()\n",
    "for x, y in train_dataloader:\n",
    "    y_pred = model_yolo(x)\n",
    "    y = y\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(133.7650, grad_fn=<AddBackward0>)"
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "f_loss = YoloLoss()\n",
    "loss = f_loss(y_pred,y)\n",
    "loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "class Fake_Net(nn.Module):  # можно поменять на Lightning\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Conv2d(in_channels=3,  out_channels = 3, kernel_size=3, stride =2)\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = nn.Flatten()(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "model_yolo = YOLO()\n",
    "f_loss = YoloLoss()\n",
    "optimizer = torch.optim.Adam( model_yolo.parameters(),lr=0.001,eps=1e-08)\n",
    "\n",
    "# fake_loss = nn.MSELoss(reduction=\"sum\")\n",
    "# fake_net = Fake_Net()\n",
    "# fake_optim = torch.optim.Adam(fake_net.parameters(), lr=1e-3, eps=1e-8 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss  tensor(4237.5059, grad_fn=<AddBackward0>)\n",
      "loss  tensor(11473.1641, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1030.8716, grad_fn=<AddBackward0>)\n",
      "loss  tensor(3069.4968, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1778.0552, grad_fn=<AddBackward0>)\n",
      "loss  tensor(5566.4761, grad_fn=<AddBackward0>)\n",
      "loss  tensor(12841.4365, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1623.9628, grad_fn=<AddBackward0>)\n",
      "loss  tensor(7613.2349, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1895.7800, grad_fn=<AddBackward0>)\n",
      "loss  tensor(3618.0898, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1085.3815, grad_fn=<AddBackward0>)\n",
      "loss  tensor(3683.2000, grad_fn=<AddBackward0>)\n",
      "loss  tensor(14529.6621, grad_fn=<AddBackward0>)\n",
      "loss  tensor(24587.8438, grad_fn=<AddBackward0>)\n",
      "loss  tensor(14471.3115, grad_fn=<AddBackward0>)\n",
      "loss  tensor(3550.0630, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1174.7939, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1400.7972, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1116.2472, grad_fn=<AddBackward0>)\n",
      "loss  tensor(911.4396, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1949.7932, grad_fn=<AddBackward0>)\n",
      "loss  tensor(498.2642, grad_fn=<AddBackward0>)\n",
      "loss  tensor(3399.5034, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1817.6404, grad_fn=<AddBackward0>)\n",
      "loss  tensor(6741.0059, grad_fn=<AddBackward0>)\n",
      "loss  tensor(2323.3660, grad_fn=<AddBackward0>)\n",
      "loss  tensor(9441.8457, grad_fn=<AddBackward0>)\n",
      "loss  tensor(2221.9648, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1632.6807, grad_fn=<AddBackward0>)\n",
      "loss  tensor(706.3794, grad_fn=<AddBackward0>)\n",
      "loss  tensor(14436.5205, grad_fn=<AddBackward0>)\n",
      "loss  tensor(3558.8577, grad_fn=<AddBackward0>)\n",
      "loss  tensor(4533.4424, grad_fn=<AddBackward0>)\n",
      "loss  tensor(2172.4001, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1977.8246, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1793.2271, grad_fn=<AddBackward0>)\n",
      "loss  tensor(26281.6602, grad_fn=<AddBackward0>)\n",
      "loss  tensor(5319.2012, grad_fn=<AddBackward0>)\n",
      "loss  tensor(5408.1914, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1760.6813, grad_fn=<AddBackward0>)\n",
      "loss  tensor(991.3465, grad_fn=<AddBackward0>)\n",
      "loss  tensor(6370.7344, grad_fn=<AddBackward0>)\n",
      "loss  tensor(2461.8164, grad_fn=<AddBackward0>)\n",
      "loss  tensor(3482.0198, grad_fn=<AddBackward0>)\n",
      "loss  tensor(3737.5293, grad_fn=<AddBackward0>)\n",
      "loss  tensor(7908.9360, grad_fn=<AddBackward0>)\n",
      "loss  tensor(4034.9119, grad_fn=<AddBackward0>)\n",
      "loss  tensor(49425.0586, grad_fn=<AddBackward0>)\n",
      "loss  tensor(2161.6631, grad_fn=<AddBackward0>)\n",
      "loss  tensor(3379.3799, grad_fn=<AddBackward0>)\n",
      "loss  tensor(6667.7363, grad_fn=<AddBackward0>)\n",
      "loss  tensor(2274.4443, grad_fn=<AddBackward0>)\n",
      "loss  tensor(2357.4893, grad_fn=<AddBackward0>)\n",
      "loss  tensor(2953.6055, grad_fn=<AddBackward0>)\n",
      "loss  tensor(6611.6240, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1630.0624, grad_fn=<AddBackward0>)\n",
      "loss  tensor(3847.1770, grad_fn=<AddBackward0>)\n",
      "loss  tensor(777.5114, grad_fn=<AddBackward0>)\n",
      "loss  tensor(940.0324, grad_fn=<AddBackward0>)\n",
      "loss  tensor(496.9366, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1643.9738, grad_fn=<AddBackward0>)\n",
      "loss  tensor(3888.4204, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1892.4762, grad_fn=<AddBackward0>)\n",
      "loss  tensor(330.0002, grad_fn=<AddBackward0>)\n",
      "loss  tensor(462.8698, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1829.5266, grad_fn=<AddBackward0>)\n",
      "loss  tensor(546.0378, grad_fn=<AddBackward0>)\n",
      "loss  tensor(698.6635, grad_fn=<AddBackward0>)\n",
      "loss  tensor(378.0261, grad_fn=<AddBackward0>)\n",
      "loss  tensor(535.7101, grad_fn=<AddBackward0>)\n",
      "loss  tensor(638.8038, grad_fn=<AddBackward0>)\n",
      "loss  tensor(1908.7156, grad_fn=<AddBackward0>)\n",
      "loss  tensor(850.1351, grad_fn=<AddBackward0>)\n",
      "loss  tensor(360.8468, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [29], line 11\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# y_pred = fake_net(x)\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# y_pred = nn.Flatten()(y_pred)\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# loss = fake_loss(y_pred,torch.zeros(y_pred.size()))\u001B[39;00m\n\u001B[1;32m     10\u001B[0m loss \u001B[38;5;241m=\u001B[39m f_loss(y_pred,y)\n\u001B[0;32m---> 11\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# print(\"y_pred \",y_pred)\u001B[39;00m\n\u001B[1;32m     14\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# та часть где я не понимаю что происходит\n",
    "for x, y in train_dataloader:\n",
    "    # model.train()\n",
    "    y_pred = model_yolo(x)\n",
    "\n",
    "    # y_pred = fake_net(x)\n",
    "    # y_pred = nn.Flatten()(y_pred)\n",
    "    # loss = fake_loss(y_pred,torch.zeros(y_pred.size()))\n",
    "\n",
    "    loss = f_loss(y_pred,y)\n",
    "    loss.backward()\n",
    "\n",
    "    # print(\"y_pred \",y_pred)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(\"loss \",loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ1eev1EeNk7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Реализуйте дополнительные функции из статьи - 2 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMF8e6yXU6QV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def non_max_suppression(bboxes, iou_threshold, threshold):\n",
    "    \"\"\"\n",
    "    Non max suppression for list of hypotheses\n",
    "\n",
    "    :bboxes: List[torch.tensor] list of coordinates of bounding boxes and there conference\n",
    "    :iou_threshold: float threshold for intersection_over_union function\n",
    "\n",
    "    :returns: List[torch.tensor] list of coordinates of bounding boxes\n",
    "    \"\"\"\n",
    "    bboxes = bboxes.sort(lambda b_box: b_box[4])\n",
    "    answ = [bboxes[0]]\n",
    "    for bbox in bboxes[1:]:\n",
    "        for answ_box in answ:\n",
    "            if intersection_over_union(bbox, answ_box) > iou_threshold and bbox[4] >= threshold:\n",
    "                answ.append(bbox)\n",
    "    return answ\n",
    "\n",
    "def mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5):\n",
    "    pass\n",
    "\n",
    "def get_bound_boxes(loader, model, iou_threshold=0.5, threshold=0.4):\n",
    "    ## YOUR CODE\n",
    "    return all_pred_boxes, all_true_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z38hYLM6haDk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Обучите модель и посчитайте метрики для задачи детекции - 2 балла \n",
    "\n",
    "Несмотря на то, что в этом блоке ничего сильно нового для вас не ожидается и за него формально дается лишь два балла - провести обучение очень важно для понимания того, насколько правильно реализована ваша модель и лосс.\n",
    "\n",
    "В процессе обучения будет видно все ли размерности совпадают, падает ли лосс и растут ли метрики целевой задачи, поэтому на практике этот пункт гораздо оказывается гораздо важнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "version = str(datetime.now().strftime(\"%H:%M(%d.%m)\"))\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "version = str(datetime.now().strftime(\"%H:%M(%d.%m)\"))\n",
    "run = wandb.init(project=\"iad_hw_2\", name=f'resnet18 version {version}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6BTNHNqtMpSi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class YOLOLearner(pl.LightningModule):\n",
    "    def __init__(self, model, loss) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx) -> torch.Tensor:\n",
    "        x,y = train_batch\n",
    "\n",
    "        y_pred = model_yolo(x)\n",
    "        loss = f_loss.forward(y_pred,y)\n",
    "        print(\"y_pred \",y_pred)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "sRl42I2xMpSi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /Users/fedor/DataspellProjects/IDA_HW_3/lightning_logs\n",
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | model | YOLO     | 55.1 M\n",
      "1 | loss  | YoloLoss | 0     \n",
      "-----------------------------------\n",
      "55.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 M    Total params\n",
      "220.208   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e72a614d0c9e41308e630e3eab124682"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'int' and 'CNNBlock'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [23], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      8\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(accelerator\u001B[38;5;241m=\u001B[39mdevice, max_epochs\u001B[38;5;241m=\u001B[39mn_epochs)\n\u001B[0;32m---> 10\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43myolo_learner\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:582\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`Trainer.fit()` requires a `LightningModule`, got: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    581\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m_lightning_module \u001B[38;5;241m=\u001B[39m model\n\u001B[0;32m--> 582\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    583\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    584\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:38\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     36\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 38\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[1;32m     41\u001B[0m     trainer\u001B[38;5;241m.\u001B[39m_call_teardown_hook()\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:624\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    617\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m ckpt_path \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresume_from_checkpoint\n\u001B[1;32m    618\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_set_ckpt_path(\n\u001B[1;32m    619\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    620\u001B[0m     ckpt_path,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    621\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    622\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    623\u001B[0m )\n\u001B[0;32m--> 624\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    626\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1061\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1057\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39mrestore_training_state()\n\u001B[1;32m   1059\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39mresume_end()\n\u001B[0;32m-> 1061\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1063\u001B[0m log\u001B[38;5;241m.\u001B[39mdetail(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1064\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_teardown()\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1140\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1138\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredicting:\n\u001B[1;32m   1139\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_predict()\n\u001B[0;32m-> 1140\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1163\u001B[0m, in \u001B[0;36mTrainer._run_train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1160\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mtrainer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m   1162\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m-> 1163\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 199\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001B[0m, in \u001B[0;36mFitLoop.advance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher\u001B[38;5;241m.\u001B[39msetup(dataloader, batch_to_device\u001B[38;5;241m=\u001B[39mbatch_to_device)\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 267\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 199\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:214\u001B[0m, in \u001B[0;36mTrainingEpochLoop.advance\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    211\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_started()\n\u001B[1;32m    213\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 214\u001B[0m         batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n\u001B[1;32m    218\u001B[0m \u001B[38;5;66;03m# update non-plateau LR schedulers\u001B[39;00m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;66;03m# update epoch-interval ones only when we are at the end of training epoch\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 199\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001B[0m, in \u001B[0;36mTrainingBatchLoop.advance\u001B[0;34m(self, kwargs)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mautomatic_optimization:\n\u001B[1;32m     85\u001B[0m     optimizers \u001B[38;5;241m=\u001B[39m _get_active_optimizers(\n\u001B[1;32m     86\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39moptimizers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39moptimizer_frequencies, kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_idx\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     87\u001B[0m     )\n\u001B[0;32m---> 88\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     90\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_loop\u001B[38;5;241m.\u001B[39mrun(kwargs)\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 199\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:200\u001B[0m, in \u001B[0;36mOptimizerLoop.advance\u001B[0;34m(self, optimizers, kwargs)\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madvance\u001B[39m(\u001B[38;5;28mself\u001B[39m, optimizers: List[Tuple[\u001B[38;5;28mint\u001B[39m, Optimizer]], kwargs: OrderedDict) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    198\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_kwargs(kwargs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer_idx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hiddens)\n\u001B[0;32m--> 200\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_optimization\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptim_progress\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_position\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mloss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    202\u001B[0m         \u001B[38;5;66;03m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001B[39;00m\n\u001B[1;32m    203\u001B[0m         \u001B[38;5;66;03m# would be skipped otherwise\u001B[39;00m\n\u001B[1;32m    204\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer_idx] \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39masdict()\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:247\u001B[0m, in \u001B[0;36mOptimizerLoop._run_optimization\u001B[0;34m(self, kwargs, optimizer)\u001B[0m\n\u001B[1;32m    239\u001B[0m         closure()\n\u001B[1;32m    241\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    242\u001B[0m \u001B[38;5;66;03m# BACKWARD PASS\u001B[39;00m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    244\u001B[0m \u001B[38;5;66;03m# gradient update with accumulated gradients\u001B[39;00m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;66;03m# the `batch_idx` is optional with inter-batch parallelism\u001B[39;00m\n\u001B[0;32m--> 247\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopt_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbatch_idx\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    249\u001B[0m result \u001B[38;5;241m=\u001B[39m closure\u001B[38;5;241m.\u001B[39mconsume_result()\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mloss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    252\u001B[0m     \u001B[38;5;66;03m# if no result, user decided to skip optimization\u001B[39;00m\n\u001B[1;32m    253\u001B[0m     \u001B[38;5;66;03m# otherwise update running loss + reset accumulated loss\u001B[39;00m\n\u001B[1;32m    254\u001B[0m     \u001B[38;5;66;03m# TODO: find proper way to handle updating running loss\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:357\u001B[0m, in \u001B[0;36mOptimizerLoop._optimizer_step\u001B[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001B[0m\n\u001B[1;32m    354\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptim_progress\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mincrement_ready()\n\u001B[1;32m    356\u001B[0m \u001B[38;5;66;03m# model hook\u001B[39;00m\n\u001B[0;32m--> 357\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_lightning_module_hook\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moptimizer_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    362\u001B[0m \u001B[43m    \u001B[49m\u001B[43mopt_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    363\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_step_and_backward_closure\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    364\u001B[0m \u001B[43m    \u001B[49m\u001B[43mon_tpu\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTPUAccelerator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    365\u001B[0m \u001B[43m    \u001B[49m\u001B[43musing_native_amp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mamp_backend\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mAMPType\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mNATIVE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[43m    \u001B[49m\u001B[43musing_lbfgs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_lbfgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    367\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m should_accumulate:\n\u001B[1;32m    370\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptim_progress\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mincrement_completed()\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1305\u001B[0m, in \u001B[0;36mTrainer._call_lightning_module_hook\u001B[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1302\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m hook_name\n\u001B[1;32m   1304\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LightningModule]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpl_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1305\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1307\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m   1308\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/core/module.py:1661\u001B[0m, in \u001B[0;36mLightningModule.optimizer_step\u001B[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001B[0m\n\u001B[1;32m   1579\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimizer_step\u001B[39m(\n\u001B[1;32m   1580\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1581\u001B[0m     epoch: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1588\u001B[0m     using_lbfgs: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1589\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1590\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1591\u001B[0m \u001B[38;5;124;03m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001B[39;00m\n\u001B[1;32m   1592\u001B[0m \u001B[38;5;124;03m    each optimizer.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1659\u001B[0m \n\u001B[1;32m   1660\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1661\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer_closure\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:169\u001B[0m, in \u001B[0;36mLightningOptimizer.step\u001B[0;34m(self, closure, **kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MisconfigurationException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_strategy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 169\u001B[0m step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_on_after_step()\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m step_output\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:234\u001B[0m, in \u001B[0;36mStrategy.optimizer_step\u001B[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;66;03m# TODO(lite): remove assertion once strategy's optimizer_step typing is fixed\u001B[39;00m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, pl\u001B[38;5;241m.\u001B[39mLightningModule)\n\u001B[0;32m--> 234\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprecision_plugin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_idx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mopt_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    236\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:121\u001B[0m, in \u001B[0;36mPrecisionPlugin.optimizer_step\u001B[0;34m(self, optimizer, model, optimizer_idx, closure, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001B[39;00m\n\u001B[1;32m    120\u001B[0m closure \u001B[38;5;241m=\u001B[39m partial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001B[0;32m--> 121\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m--> 140\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m     obj\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/optimizer.py:23\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     22\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m---> 23\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     25\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(prev_grad)\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/adam.py:183\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure, grad_scaler)\u001B[0m\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m closure \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39menable_grad():\n\u001B[0;32m--> 183\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m group \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_groups:\n\u001B[1;32m    186\u001B[0m     params_with_grad \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:107\u001B[0m, in \u001B[0;36mPrecisionPlugin._wrap_closure\u001B[0;34m(self, model, optimizer, optimizer_idx, closure)\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_closure\u001B[39m(\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     96\u001B[0m     model: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     99\u001B[0m     closure: Callable[[], Any],\n\u001B[1;32m    100\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03m    ``on_before_optimizer_step`` hook is called.\u001B[39;00m\n\u001B[1;32m    103\u001B[0m \n\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;124;03m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001B[39;00m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 107\u001B[0m     closure_result \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_closure(model, optimizer, optimizer_idx)\n\u001B[1;32m    109\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m closure_result\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:147\u001B[0m, in \u001B[0;36mClosure.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[Tensor]:\n\u001B[0;32m--> 147\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\u001B[38;5;241m.\u001B[39mloss\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:133\u001B[0m, in \u001B[0;36mClosure.closure\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclosure\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ClosureResult:\n\u001B[0;32m--> 133\u001B[0m     step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_step_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m step_output\u001B[38;5;241m.\u001B[39mclosure_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    136\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwarning_cache\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:406\u001B[0m, in \u001B[0;36mOptimizerLoop._training_step\u001B[0;34m(self, kwargs)\u001B[0m\n\u001B[1;32m    397\u001B[0m \u001B[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001B[39;00m\n\u001B[1;32m    398\u001B[0m \n\u001B[1;32m    399\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    403\u001B[0m \u001B[38;5;124;03m    A ``ClosureResult`` containing the training step output.\u001B[39;00m\n\u001B[1;32m    404\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    405\u001B[0m \u001B[38;5;66;03m# manually capture logged metrics\u001B[39;00m\n\u001B[0;32m--> 406\u001B[0m training_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtraining_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    407\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mpost_training_step()\n\u001B[1;32m    409\u001B[0m model_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_call_lightning_module_hook(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_step_end\u001B[39m\u001B[38;5;124m\"\u001B[39m, training_step_output)\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1443\u001B[0m, in \u001B[0;36mTrainer._call_strategy_hook\u001B[0;34m(self, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1440\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   1442\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1443\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1445\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m   1446\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:378\u001B[0m, in \u001B[0;36mStrategy.training_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39mtrain_step_context():\n\u001B[1;32m    377\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, TrainingStep)\n\u001B[0;32m--> 378\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [22], line 18\u001B[0m, in \u001B[0;36mYOLOLearner.training_step\u001B[0;34m(self, train_batch, batch_idx)\u001B[0m\n\u001B[1;32m     15\u001B[0m x,y \u001B[38;5;241m=\u001B[39m train_batch\n\u001B[1;32m     17\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model_yolo(x)\n\u001B[0;32m---> 18\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mf_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_pred \u001B[39m\u001B[38;5;124m\"\u001B[39m,y_pred)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "Cell \u001B[0;32mIn [20], line 176\u001B[0m, in \u001B[0;36mYoloLoss.forward\u001B[0;34m(self, predictions, target)\u001B[0m\n\u001B[1;32m    169\u001B[0m answ \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlambda_coord \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmse((mask_obj\u001B[38;5;241m*\u001B[39mbest_box\u001B[38;5;241m*\u001B[39mpredictions)[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m,[\u001B[38;5;241m1\u001B[39m]], target[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m,\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m+\u001B[39m \\\n\u001B[1;32m    170\u001B[0m        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlambda_coord \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmse((mask_obj\u001B[38;5;241m*\u001B[39mbest_box\u001B[38;5;241m*\u001B[39mpredictions)[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m,[\u001B[38;5;241m6\u001B[39m]], target[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m,\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# print(\"y\",answ)\u001B[39;00m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;66;03m# W\u001B[39;00m\n\u001B[1;32m    174\u001B[0m answ \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlambda_coord \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmse(\n\u001B[1;32m    175\u001B[0m     torch\u001B[38;5;241m.\u001B[39msign((mask_obj\u001B[38;5;241m*\u001B[39mbest_box\u001B[38;5;241m*\u001B[39mpredictions)[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m,[\u001B[38;5;241m2\u001B[39m]]) \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39msqrt(\n\u001B[0;32m--> 176\u001B[0m         torch\u001B[38;5;241m.\u001B[39mabs((mask_obj\u001B[38;5;241m*\u001B[39mbest_box\u001B[38;5;241m*\u001B[39mpredictions)[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m,[\u001B[38;5;241m2\u001B[39m]])\u001B[38;5;241m+\u001B[39m\u001B[38;5;241;43m1\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43me\u001B[49m\u001B[38;5;241m^\u001B[39m\u001B[38;5;241m3\u001B[39m),\n\u001B[1;32m    177\u001B[0m     torch\u001B[38;5;241m.\u001B[39msqrt(target[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m,\u001B[38;5;241m2\u001B[39m]))\u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlambda_coord \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmse(\n\u001B[1;32m    179\u001B[0m     torch\u001B[38;5;241m.\u001B[39msign((mask_obj\u001B[38;5;241m*\u001B[39mbest_box\u001B[38;5;241m*\u001B[39mpredictions)[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m,[\u001B[38;5;241m7\u001B[39m]]) \u001B[38;5;241m*\u001B[39mtorch\u001B[38;5;241m.\u001B[39msqrt(\n\u001B[1;32m    180\u001B[0m         torch\u001B[38;5;241m.\u001B[39mabs((mask_obj\u001B[38;5;241m*\u001B[39mbest_box\u001B[38;5;241m*\u001B[39mpredictions)[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m,[\u001B[38;5;241m7\u001B[39m]])\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m*\u001B[39me\u001B[38;5;241m^\u001B[39m\u001B[38;5;241m3\u001B[39m),\n\u001B[1;32m    181\u001B[0m     torch\u001B[38;5;241m.\u001B[39msqrt(target[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m,\u001B[38;5;241m2\u001B[39m]))\n\u001B[1;32m    183\u001B[0m \u001B[38;5;66;03m# print(\"w\",answ)\u001B[39;00m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;66;03m# H\u001B[39;00m\n\u001B[1;32m    185\u001B[0m answ \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlambda_coord \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmse(\n\u001B[1;32m    186\u001B[0m     torch\u001B[38;5;241m.\u001B[39msign((mask_obj\u001B[38;5;241m*\u001B[39mbest_box\u001B[38;5;241m*\u001B[39mpredictions)[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m,[\u001B[38;5;241m3\u001B[39m]]) \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39msqrt(\n\u001B[1;32m    187\u001B[0m         torch\u001B[38;5;241m.\u001B[39mabs((mask_obj\u001B[38;5;241m*\u001B[39mbest_box\u001B[38;5;241m*\u001B[39mpredictions)[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m,[\u001B[38;5;241m3\u001B[39m]])\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m*\u001B[39me\u001B[38;5;241m^\u001B[39m\u001B[38;5;241m3\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    191\u001B[0m         torch\u001B[38;5;241m.\u001B[39mabs((mask_obj\u001B[38;5;241m*\u001B[39mbest_box\u001B[38;5;241m*\u001B[39mpredictions)[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m,[\u001B[38;5;241m8\u001B[39m]])\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m*\u001B[39me\u001B[38;5;241m^\u001B[39m\u001B[38;5;241m3\u001B[39m),\n\u001B[1;32m    192\u001B[0m     torch\u001B[38;5;241m.\u001B[39msqrt( target[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m,\u001B[38;5;241m3\u001B[39m]))\n",
      "\u001B[0;31mTypeError\u001B[0m: unsupported operand type(s) for *: 'int' and 'CNNBlock'"
     ]
    }
   ],
   "source": [
    "model = YOLO()\n",
    "loss_function = YoloLoss()\n",
    "n_epochs = 10\n",
    "\n",
    "yolo_learner = YOLOLearner(model,loss_function )  ## YOUR CODE\n",
    "\n",
    "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "trainer = pl.Trainer(accelerator=device, max_epochs=n_epochs)\n",
    "\n",
    "trainer.fit(yolo_learner, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb7ioohR96vu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Посчитайте метрики задачи детекции на валидационной выборке\n",
    "\n",
    "Попробуйте понять насколько хороши ваши показатели. Если числа кажутся подозрительно низкими - возможно вам стоит перепроверить ваше решение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUnlNeot98un",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_YG71pYMpSi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Визуализируйте предсказанные bounding box'ы для любых пяти картинок из __валидационного__ датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgVdVzvMMpSi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image, targets = next(iter(val_dataset))\n",
    "preds = ## YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpp4jHs0MpSi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "image = torchvision.transform.ToPILImage()(image)\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "for box in targets[0]:\n",
    "    ## YOUR CODE\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])])\n",
    "\n",
    "for box in preds[0]:\n",
    "    ## YOUR CODE\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline='red')\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}