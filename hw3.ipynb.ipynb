{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKdcvMtSMpSg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "#Майнор ИАД. Домашнее задание 3. .\n",
    "\n",
    "В этом задании вы напишете и обучите свой собственный YOLO детектор. Нужно будет разобраться со статьей: понять какого формата должна быть обучающая пара (x, y), как перевести лосс из математической формулы в питоновский код - ну и конечно понять и реализовать саму архитектуру модели.\n",
    "\n",
    "Выборка на котрой мы будем обучать модель состоит из разнообразных фотографий яблок, бананов и апельсинов. Данные скачиваем [отсюда](https://drive.google.com/file/d/1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3/view?usp=sharing).\n",
    "\n",
    "Баллы за ДЗ распределены следующим образом: \n",
    "- Выборка для YoloV1 - 2 балла\n",
    "- YOLO модель - 2 балла\n",
    "- YOLO Loss - 3 балла\n",
    "- Вспомогательные функции - 2 балла\n",
    "- Обучение и расчет метрик - 2 балла\n",
    "\n",
    "Для построения и обучения можно использовать как pytorch, так и pytorch-lightning.\n",
    "\n",
    "Да-да, баллов в сумме получается 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TY4niK1xMpSg",
    "outputId": "1810d5cf-5d4d-4b5b-db31-5a220ee2b74d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (0.13.0)\r\n",
      "Requirement already satisfied: pytorch-lightning in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (1.8.3.post1)\r\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (6.0)\r\n",
      "Requirement already satisfied: tensorboardX>=2.2 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (2.5.1)\r\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (2022.10.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (1.21.5)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (4.3.0)\r\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (4.64.1)\r\n",
      "Requirement already satisfied: torch>=1.9.* in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (1.13.0)\r\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (21.3)\r\n",
      "Requirement already satisfied: lightning-utilities==0.3.* in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (0.3.0)\r\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning) (0.11.0)\r\n",
      "Requirement already satisfied: fire in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from lightning-utilities==0.3.*->pytorch-lightning) (0.4.0)\r\n",
      "Requirement already satisfied: requests in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.28.1)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\r\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from tensorboardX>=2.2->pytorch-lightning) (3.20.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.2)\r\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.1)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.1.0)\r\n",
      "Requirement already satisfied: termcolor in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from fire->lightning-utilities==0.3.*->pytorch-lightning) (1.1.0)\r\n",
      "Requirement already satisfied: six in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from fire->lightning-utilities==0.3.*->pytorch-lightning) (1.16.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.12)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fedor/opt/anaconda3/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.9.24)\r\n"
     ]
    }
   ],
   "source": [
    "# Данная библиотека понадобится нам, чтобы обработать разметку\n",
    "! pip install xmltodict pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNSQ7FNss30F",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Скачаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lN1dE6eY7PjV",
    "outputId": "beb005ec-2b9b-42ab-c294-127ee4ec4b39",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 64\r\n",
      "drwxr-xr-x  4 fedor  staff    128 Dec  1 12:27 \u001B[34m__MACOSX\u001B[m\u001B[m\r\n",
      "drwx------  5 fedor  staff    160 Nov 26 11:32 \u001B[34mdata\u001B[m\u001B[m\r\n",
      "-rw-r--r--  1 fedor  staff  31386 Dec  1 12:27 hw3.ipynb.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!wget --quiet --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://drive.google.com/uc?export=download&id=1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3\" -O data.zip && rm -rf /tmp/cookies.txt\n",
    "!unzip -q data.zip\n",
    "!rm data.zip\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ep38vdW_s-Rz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Посмотрим как выглядит один из файлов разметки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OqwaHl3ntBaN",
    "outputId": "77ce5c63-e716-4a57-90a9-2179534fa789",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<annotation>\r\n",
      "\t<folder>train</folder>\r\n",
      "\t<filename>apple_3.jpg</filename>\r\n",
      "\t<path>C:\\tensorflow1\\models\\research\\object_detection\\images\\train\\apple_3.jpg</path>\r\n",
      "\t<source>\r\n",
      "\t\t<database>Unknown</database>\r\n",
      "\t</source>\r\n",
      "\t<size>\r\n",
      "\t\t<width>1000</width>\r\n",
      "\t\t<height>708</height>\r\n",
      "\t\t<depth>3</depth>\r\n",
      "\t</size>\r\n",
      "\t<segmented>0</segmented>\r\n",
      "\t<object>\r\n",
      "\t\t<name>apple</name>\r\n",
      "\t\t<pose>Unspecified</pose>\r\n",
      "\t\t<truncated>1</truncated>\r\n",
      "\t\t<difficult>0</difficult>\r\n",
      "\t\t<bndbox>\r\n",
      "\t\t\t<xmin>584</xmin>\r\n",
      "\t\t\t<ymin>438</ymin>\r\n",
      "\t\t\t<xmax>867</xmax>\r\n",
      "\t\t\t<ymax>708</ymax>\r\n",
      "\t\t</bndbox>\r\n",
      "\t</object>\r\n",
      "\t<object>\r\n",
      "\t\t<name>apple</name>\r\n",
      "\t\t<pose>Unspecified</pose>\r\n",
      "\t\t<truncated>0</truncated>\r\n",
      "\t\t<difficult>0</difficult>\r\n",
      "\t\t<bndbox>\r\n",
      "\t\t\t<xmin>492</xmin>\r\n",
      "\t\t\t<ymin>141</ymin>\r\n",
      "\t\t\t<xmax>740</xmax>\r\n",
      "\t\t\t<ymax>394</ymax>\r\n",
      "\t\t</bndbox>\r\n",
      "\t</object>\r\n",
      "\t<object>\r\n",
      "\t\t<name>apple</name>\r\n",
      "\t\t<pose>Unspecified</pose>\r\n",
      "\t\t<truncated>0</truncated>\r\n",
      "\t\t<difficult>0</difficult>\r\n",
      "\t\t<bndbox>\r\n",
      "\t\t\t<xmin>176</xmin>\r\n",
      "\t\t\t<ymin>199</ymin>\r\n",
      "\t\t\t<xmax>490</xmax>\r\n",
      "\t\t\t<ymax>466</ymax>\r\n",
      "\t\t</bndbox>\r\n",
      "\t</object>\r\n",
      "\t<object>\r\n",
      "\t\t<name>apple</name>\r\n",
      "\t\t<pose>Unspecified</pose>\r\n",
      "\t\t<truncated>0</truncated>\r\n",
      "\t\t<difficult>0</difficult>\r\n",
      "\t\t<bndbox>\r\n",
      "\t\t\t<xmin>367</xmin>\r\n",
      "\t\t\t<ymin>17</ymin>\r\n",
      "\t\t\t<xmax>619</xmax>\r\n",
      "\t\t\t<ymax>240</ymax>\r\n",
      "\t\t</bndbox>\r\n",
      "\t</object>\r\n",
      "\t<object>\r\n",
      "\t\t<name>apple</name>\r\n",
      "\t\t<pose>Unspecified</pose>\r\n",
      "\t\t<truncated>0</truncated>\r\n",
      "\t\t<difficult>0</difficult>\r\n",
      "\t\t<bndbox>\r\n",
      "\t\t\t<xmin>642</xmin>\r\n",
      "\t\t\t<ymin>35</ymin>\r\n",
      "\t\t\t<xmax>907</xmax>\r\n",
      "\t\t\t<ymax>269</ymax>\r\n",
      "\t\t</bndbox>\r\n",
      "\t</object>\r\n",
      "</annotation>\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/train/apple_3.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdQxrA5_MpSg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Релизуйте выборку для YoloV1 - 2 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1247,
   "metadata": {
    "id": "QXG9reop-BkS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "import tqdm\n",
    "import xmltodict\n",
    "\n",
    "from IPython.core.display import struct\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "# Добавьте необходимые вам библиотеки, если их не окажется в списке выше\n",
    "\n",
    "import xml.etree.ElementTree as ET #for xml parsing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gL8_CyyTYJ-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Так как в этом домашнем задании использовать аугментации для обучения __обязательно__ - советуем воспользоваться библиотекой albumentations.\n",
    "\n",
    "Она  особенно удобна, поскольку умеет сама вычислять новые координаты bounding box'ов после трансформаций картинки. Для знакомства с этим механизмом советуем следующий [гайд](hts://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/).\n",
    "\n",
    "Вы все еще можете избрать путь torchvision.transforms, вам потребуется знакомый нам метод `__getitem__`, однако вычислять новые координаты bounding box'ов после трансформаций вам придётся вручную\n",
    "\n",
    "__Обратите внимание__ на то, что в статье коробки предсказаний параметризуются через: _(x_center, y_center, width, height)_ (причем эти значения _относительные_), а в наших файлах - это _(x_min, y_min, x_max, y_max)_\n",
    "\n",
    "Также, помните что модель должна предсказывать как прямоугольник с обьектом, так и вероятности каждого класса!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def biection(data_dir):\n",
    "    \"\"\"\"\n",
    "    Сопоставляет каждой картинке и таблице в папке номер\n",
    "    -------------------------\n",
    "    return: словарь с ключами - названием картинки/таблицы, объектами - номерами\n",
    "    \"\"\"\n",
    "    w_path = os.path.join(os.getcwd(),data_dir)\n",
    "    answ = {}\n",
    "    count = 0\n",
    "    for filename in os.listdir(w_path):\n",
    "        name =  filename.split(\".\")[0]\n",
    "        if name not in answ.keys():\n",
    "            answ[name] = count\n",
    "            count += 1\n",
    "    return answ\n",
    "\n",
    "2 * len(biection(\"./data/test\")) == len(os.listdir(os.path.join(os.getcwd(),\"./data/test\")))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "def get_paths(names_to_num, data_dir):\n",
    "    \"\"\"\"\n",
    "    Возвращает словарь путей до картинок и таблиц с коробками, причём\n",
    "    индексация в различных словарях совпадает\n",
    "    -------------------------\n",
    "    return: словарь с ключами - цифрами, объектами - ссылки\n",
    "    \"\"\"\n",
    "    w_path = os.path.join(os.getcwd(),data_dir)\n",
    "    i_path = {}\n",
    "    b_path = {}\n",
    "    for name in list(names_to_num.keys()):\n",
    "        num = names_to_num[name]\n",
    "        i_path[num] = os.path.join(w_path,(name + \".jpg\"))\n",
    "        b_path[num] = os.path.join(w_path,(name + \".xml\"))\n",
    "    return i_path, b_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "path_1 = '/Users/fedor/DataspellProjects/IDA_HW_3/data/test/orange_77.jpg'\n",
    "h = np.array(Image.open(path_1).convert(\"RGB\")).shape[0]\n",
    "w = np.array(Image.open(path_1).convert(\"RGB\")).shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "id": "tjZkU0vzMpSh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class2tag = {\"apple\": 1, \"orange\": 2, \"banana\": 3}\n",
    "\n",
    "class FruitDataset(Dataset):\n",
    "    def __init__(self, data_dir, transforms=None):\n",
    "        self.biection = biection(data_dir)\n",
    "        self.image_paths , self.box_paths = get_paths(self.biection, data_dir)\n",
    "        assert len(self.image_paths) == len(self.box_paths)\n",
    "        self.transforms = transforms\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array( Image.open(self.image_paths[idx]).convert(\"RGB\") )\n",
    "        bboxes, class_labels = self.__get_boxes_from_xml(self.box_paths[idx])\n",
    "        bboxes = (\n",
    "            self.__convert_to_yolo_box_params(\n",
    "                                        bboxes,\n",
    "                                        image.shape[1],\n",
    "                                        image.shape[0])\n",
    "                                                )\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image = image, bboxes = bboxes, class_labels = class_labels)\n",
    "            bboxes = transformed['bboxes']\n",
    "            image = transformed['image']\n",
    "            class_labels =  transformed['class_labels']\n",
    "\n",
    "        return torch.Tensor(image).permute(2, 0, 1), (bboxes, class_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __get_boxes_from_xml(self, xml_filename: str):\n",
    "        \"\"\"\n",
    "          Метод, который считает и распарсит (с помощью xmltodict) переданный xml\n",
    "          файл и вернет координаты прямоугольников объектов на соответсвующей фотографии\n",
    "          и название класса обьекта в каждом прямоугольнике\n",
    "        --------------------------\n",
    "        return:\n",
    "        boxes - format lists of lists [xmin, ymin, xmax, ymax]\n",
    "        name - list of\n",
    "        \"\"\"\n",
    "        root_node = ET.parse(xml_filename).getroot()\n",
    "        boxes = []\n",
    "        class_labels = []\n",
    "\n",
    "        for object in root_node.iter('object'):# Get the value from the attribute 'object'\n",
    "            name = object.find('name').text\n",
    "            xmin = int(object.find('bndbox').find('xmin').text)\n",
    "            ymin = int(object.find('bndbox').find('ymin').text)\n",
    "            xmax=  int(object.find('bndbox').find('xmax').text)\n",
    "            ymax = int(object.find('bndbox').find('ymax').text)\n",
    "\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            class_labels.append(class2tag[name])\n",
    "\n",
    "        return boxes, class_labels\n",
    "\n",
    "    def __convert_to_yolo_box_params(self, all_box_coordinates: List[int], im_w, im_h):\n",
    "        \"\"\"\n",
    "        Перейти от [xmin, ymin, xmax, ymax] к [x_center, y_center, width, height].\n",
    "\n",
    "        Обратите внимание, что параметры [x_center, y_center, width, height] - это\n",
    "        относительные значение в отрезке [0, 1]\n",
    "\n",
    "        :param: box_coordinates - координаты коробки в формате [xmin, ymin, xmax, ymax]\n",
    "        :param: im_w - ширина исходного изображения\n",
    "        :param: im_h - высота исходного изображения\n",
    "\n",
    "        :return: координаты коробки в формате [x_center, y_center, width, height]\n",
    "        \"\"\"\n",
    "        totall_anw = []\n",
    "        for box_coordinates in all_box_coordinates:\n",
    "            ans = []\n",
    "            ans.append((box_coordinates[0] + box_coordinates[2]) / 2 / im_w)  # x_center\n",
    "            ans.append((box_coordinates[1] + box_coordinates[3]) / 2 / im_h)  # y_center\n",
    "\n",
    "            ans.append((box_coordinates[2] - box_coordinates[0]) / im_w)  # width\n",
    "            ans.append((box_coordinates[3] - box_coordinates[1]) / im_h)  # height\n",
    "            totall_anw.append(ans)\n",
    "\n",
    "        return totall_anw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "id": "OwXeSiAjdGeq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "WIDTH, HEIGHT = 448, 448\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    albumentations.augmentations.geometric.resize.Resize(height=HEIGHT, width = WIDTH),\n",
    "    A.RandomRotate90(p=0.3),\n",
    "    A.HorizontalFlip(p=0.3),\n",
    "    A.Transpose(p=0.3),\n",
    "    A.PixelDropout(p=0.5),\n",
    "    A.Blur(blur_limit=(3,4), p=0.5)\n",
    "                            ],\n",
    "                             bbox_params=A.BboxParams(format='yolo',\n",
    "                                                      label_fields=['class_labels']))\n",
    "test_transform = A.Compose([\n",
    "    albumentations.augmentations.geometric.resize.Resize(height=HEIGHT, width = WIDTH)\n",
    "                            ],\n",
    "                            bbox_params=A.BboxParams(format='yolo',\n",
    "                                                     label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ayPwbRKocdCE",
    "outputId": "c7ec7134-c763-435c-a575-de2e16122a82",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты успешно пройдены\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FruitDataset(\n",
    "    transforms=train_transform,\n",
    "    data_dir=\"./data/train\"\n",
    "    )\n",
    "\n",
    "val_dataset = FruitDataset(\n",
    "    transforms=test_transform, \n",
    "    data_dir=\"./data/test\"\n",
    "    )\n",
    "\n",
    "# Немного проверок, чтобы убедиться в правильности направления решения\n",
    "assert isinstance(train_dataset[0], tuple)\n",
    "assert len(train_dataset[0]) == 2\n",
    "assert isinstance(train_dataset[0][0], torch.Tensor)\n",
    "assert isinstance(train_dataset[0][1], tuple)\n",
    "assert len(train_dataset[0][1]) == 2\n",
    "print(\"Тесты успешно пройдены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "metadata": {
    "id": "9V1Tl_GAdeIv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size= 1,\n",
    "    shuffle=True)\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fRR9ns6MpSh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Теперь определим функцию для рассчета Intersection Over Union по 4 углам двух прямоугольников"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {
    "id": "Rd88hnZiMpSh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(predicted_bbox, gt_bbox) -> float:\n",
    "    \"\"\"\n",
    "    Intersection Over Union для двух прямоугольников\n",
    "\n",
    "    :param: dt_bbox - [x_min, y_min, x_max, y_max]\n",
    "    :param: gt_bbox - [x_min, y_min, x_max, y_max]\n",
    "\n",
    "    :return: Intersection Over Union\n",
    "    \"\"\"\n",
    "\n",
    "    # intersection_bbox = np.array(\n",
    "    intersection_bbox = torch.Tensor(\n",
    "        [\n",
    "            max(predicted_bbox[0], gt_bbox[0]),\n",
    "            max(predicted_bbox[1], gt_bbox[1]),\n",
    "            min(predicted_bbox[2], gt_bbox[2]),\n",
    "            min(predicted_bbox[3], gt_bbox[3]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    intersection_area = max(intersection_bbox[2] - intersection_bbox[0], 0) \\\n",
    "                        * max( intersection_bbox[3] - intersection_bbox[1], 0)\n",
    "    area_dt = (predicted_bbox[2] - predicted_bbox[0]) * (predicted_bbox[3] - predicted_bbox[1])\n",
    "    area_gt = (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1])\n",
    "\n",
    "    union_area = area_dt + area_gt - intersection_area\n",
    "\n",
    "    iou = intersection_area / union_area\n",
    "    return float(iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVJWo3xbMpSh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Теперь начинается основная часть домашнего задания: обучите модель YOLO для object detection на __обучающем__ датасете. \n",
    "\n",
    " - Создайте модель и функцию ошибки YoloV1 прочитав [оригинальную статью](https://paperswithcode.com/paper/you-only-look-once-unified-real-time-object)\n",
    " - Напишите функцию обучения модели\n",
    " - Используйте аугментации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxfMVwzHW2MJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Реализуйте Модель - 2 балла\n",
    "\n",
    "Копировать точное количество слоев и параметры сверток необязательно. Главное - чтобы модель работала по принципу, описанному в статье и делала предсказание в представленном формате.\n",
    "\n",
    "\n",
    "В качестве подсказки напомним, что выходом модели __для каждого обьекта__ должен быть тензор размера\n",
    "__S * S * (B * 5 + С)__, где все параметры имеют такое же значение, как и в статье: \n",
    "\n",
    "- S - количество ячеек на которое разбивается изображение по вертикали/горизонтали\n",
    "- В - количество предсказываемых прямоугольников в каждой ячейке\n",
    "- 5 - количество параметров для определения каждого прямоугольника (x_center, y_center, width, height, confidence)\n",
    "- С - количество классов (apple, banana, orange)\n",
    "\n",
    "Таким образом, мы для каждого окна размера __S x S__ предсказываем __В__ коробо и один класс"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1536,
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):  # можно поменять на Lightning\n",
    "    def __init__(self, convs,out_channels, is_max_pool:bool=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv_layers = convs\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "        self.is_maxpool = is_max_pool  # не после каждой свертки нужно делать maxpool\n",
    "        self.maxpool = nn.MaxPool2d(2,stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"begin\",x.size())\n",
    "        x = self.conv_layers(x)\n",
    "        # print(\"conv_layers\",x)\n",
    "        if len(x.size()) == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.leakyrelu(self.batchnorm(x))\n",
    "\n",
    "        if self.is_maxpool:\n",
    "            x = self.maxpool(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1537,
   "outputs": [],
   "source": [
    "conv_layer_1 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3,  out_channels = 192, kernel_size=3, stride =2, padding=1))\n",
    "\n",
    "conv_layer_2 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=192, out_channels=256, kernel_size=3, padding=1))\n",
    "\n",
    "conv_layer_3 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n",
    "    nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "    nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1),\n",
    "    nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    ")\n",
    "\n",
    "conv_layer_4 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),#1\n",
    "    nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "    nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),#2\n",
    "    nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "    nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),#3\n",
    "    nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "    nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),#4\n",
    "    nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "\n",
    "    nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1),\n",
    "    nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1)\n",
    ")\n",
    "\n",
    "conv_layer_5 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1),#1\n",
    "    nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1),\n",
    "    nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1),#2\n",
    "    nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1),\n",
    "\n",
    "    nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1),\n",
    "    nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3,stride = 2, padding=1)\n",
    ")\n",
    "\n",
    "conv_layer_6 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1),\n",
    "    nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1538,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "tensor([[[-3.2197e-01,  7.1361e-02, -1.6071e-02, -4.8785e-02,  7.6310e-02,\n",
      "           4.9501e-03,  6.7816e-02, -7.5816e-02, -1.0489e-01, -4.4289e-02,\n",
      "           1.1723e-02, -2.0340e-01, -1.8150e-01],\n",
      "         [ 2.8514e-01, -1.8554e-01,  9.1828e-02, -6.9996e-02, -2.2433e-01,\n",
      "          -4.1195e-04,  1.9483e-01,  7.4777e-02, -6.5894e-03, -2.1164e-01,\n",
      "           2.5075e-02,  7.1717e-02,  1.7405e-01],\n",
      "         [ 1.8891e-01, -1.1470e-01, -1.7718e-01, -5.1096e-02, -7.9399e-03,\n",
      "           2.9963e-01, -6.4960e-02, -3.7586e-02, -5.1841e-02,  1.5236e-01,\n",
      "           4.7161e-02, -1.2270e-01, -4.2680e-01],\n",
      "         [ 1.3315e-01,  2.6852e-01,  5.1177e-02,  2.7665e-01,  4.6333e-03,\n",
      "           2.8169e-02, -8.0084e-02, -1.5513e-02, -1.4250e-01,  2.6033e-01,\n",
      "          -1.3737e-02, -1.2552e-02, -6.1582e-02],\n",
      "         [-2.4519e-01,  3.1792e-02, -9.6789e-02, -8.3163e-02,  7.3554e-03,\n",
      "          -2.3819e-01, -2.0936e-01, -4.0170e-02, -2.2668e-03,  1.0548e-01,\n",
      "           1.6951e-01, -3.9834e-02,  7.4657e-02],\n",
      "         [ 7.5818e-02, -5.2999e-02,  1.7790e-01, -2.9123e-02, -3.8328e-01,\n",
      "           1.5449e-01, -5.7312e-02,  1.2389e-01, -9.1949e-03,  3.9133e-02,\n",
      "          -1.4679e-01,  7.1205e-03,  1.0227e-02],\n",
      "         [ 1.0279e-01, -5.9579e-02,  1.9326e-01,  2.8106e-01,  1.2125e-01,\n",
      "           1.6669e-02,  6.6440e-03, -6.8803e-02, -1.7224e-02,  2.8118e-01,\n",
      "           3.4770e-02, -9.2987e-02,  5.9329e-02]],\n",
      "\n",
      "        [[-1.1646e-01,  1.4574e-01,  7.6770e-02, -5.2392e-01,  2.1170e-01,\n",
      "           3.0154e-01, -1.0665e-01, -6.1486e-02,  3.4328e-02,  9.5714e-02,\n",
      "           3.0303e-01,  1.5685e-01, -9.1205e-02],\n",
      "         [-3.7727e-02,  6.0296e-03, -1.6899e-01, -3.8358e-01,  3.4003e-01,\n",
      "           1.2761e-01, -1.1032e-01,  1.1167e-01,  1.9774e-01,  5.0646e-02,\n",
      "          -1.0970e-01,  9.6618e-02,  4.5557e-02],\n",
      "         [-1.7293e-02,  3.0657e-02, -1.9887e-01,  2.1122e-01, -2.2095e-01,\n",
      "          -2.8222e-01, -1.7432e-01,  1.8382e-01,  4.9007e-02, -1.4015e-01,\n",
      "           3.7235e-01, -1.5579e-01, -4.1218e-02],\n",
      "         [-1.6127e-01, -9.1369e-02,  2.0264e-02,  5.0499e-02, -1.3377e-02,\n",
      "           8.9245e-02, -7.0304e-02,  9.0766e-02,  5.0766e-02, -9.8392e-02,\n",
      "           2.5025e-01, -6.5163e-02,  1.0998e-01],\n",
      "         [ 1.2550e-01, -2.9597e-01, -6.5758e-02, -4.3953e-02,  6.7684e-02,\n",
      "           2.8087e-01, -2.0280e-01,  1.1218e-02,  1.9821e-01, -2.6204e-02,\n",
      "           1.7752e-01, -4.3011e-02, -6.2858e-02],\n",
      "         [-1.8461e-01,  4.2538e-02,  3.8380e-02, -4.1273e-01, -3.1584e-01,\n",
      "           3.2963e-01, -2.5093e-01, -3.3221e-02,  3.1285e-01, -5.7115e-02,\n",
      "          -5.6694e-01,  3.4594e-01, -8.2803e-02],\n",
      "         [ 2.3768e-01,  1.1076e-01,  3.0905e-01, -4.4562e-02,  1.9156e-01,\n",
      "          -1.1799e-02, -1.1901e-02, -7.5955e-02,  1.7475e-01, -2.9540e-01,\n",
      "           1.7089e-01, -7.2888e-02, -2.2511e-02]],\n",
      "\n",
      "        [[-8.5789e-02, -1.5450e-03, -3.2989e-01, -1.5842e-01,  2.1446e-01,\n",
      "          -2.7506e-02,  1.2908e-01, -1.0863e-01,  7.3716e-02,  3.8503e-01,\n",
      "          -6.5273e-02, -3.1856e-02,  7.4135e-02],\n",
      "         [-1.6894e-01,  9.4422e-02,  3.1409e-02, -2.3740e-01, -1.3416e-01,\n",
      "           2.0681e-01, -8.2546e-02, -3.2208e-02,  2.4913e-01,  1.0387e-02,\n",
      "          -1.9772e-01, -2.8903e-02,  8.7848e-02],\n",
      "         [ 5.1801e-02,  1.2326e-01, -1.4466e-01,  2.9322e-01, -2.8548e-01,\n",
      "          -7.2754e-02,  9.4974e-02, -4.2273e-02, -1.0891e-01, -3.0965e-01,\n",
      "           1.2740e-01,  2.1496e-01,  1.2164e-01],\n",
      "         [ 1.1289e-01,  1.5420e-01, -7.7779e-02,  8.4193e-02, -1.6438e-01,\n",
      "           2.7841e-01,  8.9062e-03, -6.1377e-02,  2.6612e-01,  1.7388e-02,\n",
      "          -2.0852e-01, -1.8802e-02,  2.4775e-01],\n",
      "         [ 9.4887e-02,  5.9964e-02, -1.2491e-01,  5.1215e-02,  1.8193e-01,\n",
      "          -3.5677e-01, -4.4810e-01, -1.2764e-01, -5.1943e-02,  1.8889e-02,\n",
      "           2.5741e-01,  2.9416e-01,  2.4266e-01],\n",
      "         [-5.4172e-02, -4.1867e-02,  4.0871e-02, -3.1487e-02,  2.1117e-01,\n",
      "           1.7803e-02,  2.7244e-01,  1.6423e-01,  1.0085e-01,  5.0239e-02,\n",
      "          -1.9434e-01,  1.1764e-01, -1.0766e-01],\n",
      "         [ 2.0737e-02, -4.2641e-02, -3.2094e-02,  5.5969e-03, -1.0561e-01,\n",
      "           1.7946e-01,  2.5880e-02,  1.8593e-01, -2.7255e-01,  1.3647e-01,\n",
      "          -9.0778e-02, -2.3642e-01,  2.0303e-01]],\n",
      "\n",
      "        [[-1.7529e-02, -1.3251e-01,  4.0010e-02,  2.4096e-01,  1.7464e-01,\n",
      "           2.1764e-02, -6.4854e-02,  1.8315e-01,  1.8288e-02, -4.4318e-02,\n",
      "          -1.7024e-02, -4.3724e-02,  1.8288e-02],\n",
      "         [-9.1264e-02,  2.1985e-01,  1.0609e-01,  1.2870e-01, -3.7526e-02,\n",
      "           8.2542e-02,  9.4432e-02,  8.2025e-02,  1.9755e-01,  2.3186e-01,\n",
      "          -1.3381e-01,  2.9072e-01,  1.8009e-01],\n",
      "         [ 8.0690e-02, -2.5671e-01, -1.8393e-01, -3.2690e-01, -3.0440e-01,\n",
      "          -1.2953e-01, -1.7809e-01,  1.8952e-02, -2.4553e-01, -1.0956e-01,\n",
      "           9.0008e-02,  8.3604e-02, -1.0189e-01],\n",
      "         [ 5.4142e-02,  2.7498e-01,  2.0337e-02, -1.6826e-02, -1.1771e-01,\n",
      "           2.7930e-01, -2.1045e-01,  4.0940e-02, -7.5093e-02, -1.9867e-01,\n",
      "           4.4682e-02, -4.2623e-02,  7.2332e-02],\n",
      "         [-5.7264e-02,  1.0311e-01, -6.4599e-02,  2.7591e-01, -2.1888e-01,\n",
      "          -1.2864e-01, -7.6479e-03, -1.5753e-03, -2.0081e-03,  8.7121e-02,\n",
      "           1.1659e-01, -4.6383e-02, -1.4017e-02],\n",
      "         [-8.7677e-02,  2.3088e-02,  2.3474e-01,  1.2249e-01,  2.0651e-01,\n",
      "          -5.6133e-02,  3.8287e-02, -8.7863e-02, -2.5864e-02, -9.7595e-02,\n",
      "          -2.2960e-01,  1.0979e-01, -2.1772e-01],\n",
      "         [ 5.0869e-02,  1.2540e-01, -1.2749e-01, -7.0312e-03,  1.4237e-01,\n",
      "           8.0835e-02, -1.0093e-01,  7.9330e-02,  4.4835e-02,  1.7327e-01,\n",
      "          -1.0497e-01,  2.1728e-02,  1.2319e-01]],\n",
      "\n",
      "        [[ 7.2596e-02,  1.9246e-02,  1.3402e-01, -6.3998e-03,  2.4559e-01,\n",
      "           2.0410e-01, -1.7419e-01,  2.2037e-01,  1.1170e-01, -1.3554e-01,\n",
      "           2.0032e-01, -1.0042e-01, -1.4060e-01],\n",
      "         [-1.9377e-01,  2.4912e-01,  4.1335e-02, -1.7791e-01, -2.1964e-01,\n",
      "           2.9876e-02,  2.4159e-01, -3.6757e-02,  6.7551e-02,  3.2076e-01,\n",
      "           1.2507e-01, -1.6820e-01, -2.2095e-01],\n",
      "         [-9.8093e-02,  1.5893e-02, -3.9365e-02, -1.8399e-01, -1.3513e-01,\n",
      "          -9.3290e-02,  5.0724e-02, -1.7119e-01, -1.2889e-01, -1.0651e-01,\n",
      "           1.0002e-01,  3.5102e-02,  2.4801e-02],\n",
      "         [-5.1512e-02, -1.1436e-01, -1.5262e-01,  2.2790e-01,  1.6084e-01,\n",
      "           1.3587e-03, -6.3689e-02, -8.7770e-02,  1.4085e-01,  1.3044e-01,\n",
      "           6.0163e-02,  3.8230e-01,  1.8823e-01],\n",
      "         [-4.8594e-02,  4.2286e-02,  1.8667e-01, -8.8455e-02, -3.0915e-02,\n",
      "          -1.4082e-01,  1.7286e-01, -5.7706e-02,  1.6559e-01,  5.7737e-02,\n",
      "          -1.3013e-01, -2.0884e-01,  1.7937e-01],\n",
      "         [-1.5763e-01,  2.3669e-01, -1.1045e-01,  1.1143e-01, -1.4285e-02,\n",
      "          -4.9736e-02,  4.9370e-02, -2.6917e-01, -1.9529e-01,  1.4072e-01,\n",
      "          -1.1500e-01,  8.2355e-02, -2.5847e-01],\n",
      "         [-1.8692e-01,  3.4382e-03,  2.7781e-01, -5.1128e-02,  3.5281e-02,\n",
      "           2.7129e-02,  1.1585e-01,  1.9686e-02,  2.0065e-01, -4.7905e-02,\n",
      "           1.4784e-01, -2.6123e-01, -2.2844e-02]],\n",
      "\n",
      "        [[-9.1336e-03,  6.1840e-02,  1.9850e-01, -2.7025e-01,  4.5439e-01,\n",
      "           1.6735e-01,  1.5716e-01, -4.0398e-02,  3.4435e-01,  1.0013e-02,\n",
      "           1.0352e-01,  1.9911e-03, -2.5825e-01],\n",
      "         [-4.3416e-01, -7.1761e-02, -4.4568e-03, -1.3551e-01, -7.6524e-02,\n",
      "          -1.4648e-01, -1.5405e-02,  1.1693e-01, -2.4764e-01, -1.1905e-01,\n",
      "          -1.6933e-01, -1.8763e-01, -2.7955e-01],\n",
      "         [-2.7194e-01,  2.5829e-01,  3.2514e-01,  1.2598e-01, -3.2833e-02,\n",
      "          -5.0300e-02, -3.1026e-01, -1.1208e-01,  1.2682e-01,  2.8702e-02,\n",
      "           2.8690e-01,  3.8102e-02, -6.4487e-02],\n",
      "         [ 2.5600e-01, -1.4935e-01,  1.5467e-01, -2.4154e-01, -3.4720e-01,\n",
      "          -2.8882e-02,  9.2351e-02,  1.4587e-01,  6.6988e-02,  1.0512e-01,\n",
      "           3.8752e-01, -1.6239e-01,  3.0185e-01],\n",
      "         [-1.1329e-01,  1.1743e-01,  2.1886e-02,  1.8061e-01, -4.6540e-02,\n",
      "           1.0083e-01,  1.2815e-01, -8.5649e-02,  3.8283e-02,  1.2116e-01,\n",
      "          -2.2196e-01, -1.1539e-01, -1.2281e-01],\n",
      "         [-2.7221e-01,  1.4063e-01, -3.3964e-01,  2.7882e-01, -3.2590e-01,\n",
      "           4.6389e-02,  2.1861e-01, -5.1073e-02, -5.9578e-02,  4.2983e-01,\n",
      "          -1.7903e-02,  2.3126e-01,  2.3648e-01],\n",
      "         [-4.7385e-02,  2.1348e-01, -2.2486e-01, -1.2750e-01,  2.3601e-01,\n",
      "          -4.1833e-02, -8.4940e-03,  7.5801e-02,  5.0021e-02,  2.1199e-03,\n",
      "          -2.8277e-01,  8.5047e-02, -3.8813e-02]],\n",
      "\n",
      "        [[ 4.1523e-02, -2.8909e-01, -3.8188e-01,  3.4388e-02, -8.5946e-02,\n",
      "          -1.0010e-01,  1.1810e-02, -1.2284e-01,  2.1400e-01,  7.7322e-02,\n",
      "           1.8746e-01, -6.1849e-02,  3.8216e-01],\n",
      "         [-2.4861e-02,  2.6534e-02,  1.0524e-01, -4.4183e-04,  3.3499e-01,\n",
      "          -9.9749e-02,  1.9151e-01,  1.8047e-02, -2.3987e-01, -2.8066e-02,\n",
      "          -1.6313e-01, -1.7062e-01,  3.3299e-01],\n",
      "         [-2.6434e-01, -2.0916e-01, -1.3715e-01,  1.5242e-01,  8.6258e-02,\n",
      "           1.7244e-02, -4.3252e-02, -2.7506e-03,  1.9772e-01, -7.0240e-02,\n",
      "          -5.7641e-02, -2.8608e-02,  8.6260e-02],\n",
      "         [-1.2555e-01,  2.8251e-02,  8.7835e-02,  4.0708e-02,  2.1414e-02,\n",
      "          -3.0766e-01,  2.0522e-01,  2.8771e-01,  2.7272e-01,  7.3657e-02,\n",
      "           1.6041e-01,  1.5997e-01,  5.2185e-02],\n",
      "         [-1.1051e-01, -2.0751e-01,  1.7014e-01,  4.1909e-02, -5.0332e-02,\n",
      "           1.9780e-01,  1.2601e-01,  4.6476e-02,  5.2036e-02, -1.2452e-01,\n",
      "          -1.4922e-01,  7.4212e-02,  4.1117e-01],\n",
      "         [ 1.1414e-01,  2.0368e-01,  1.2125e-03,  2.2403e-01,  4.6983e-02,\n",
      "           5.7346e-02,  1.9014e-01, -1.2295e-01, -1.8141e-02,  8.9982e-02,\n",
      "          -1.2307e-01,  4.3020e-02,  6.9143e-02],\n",
      "         [-3.5292e-02, -1.1888e-01, -2.4451e-02,  1.3243e-01,  5.9956e-02,\n",
      "          -7.2475e-03, -6.2325e-02,  3.8723e-02,  2.2645e-01,  1.3254e-01,\n",
      "           1.7371e-01,  2.1937e-01, -2.5240e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a  = CNNBlock(conv_layer_1, 192, True)\n",
    "x = a.forward(train_dataset.__getitem__(idx = 0)[0])\n",
    "print(x.size() == torch.Size([1, 192, 112, 112]))\n",
    "b = CNNBlock(conv_layer_2, 256, True)\n",
    "x = b.forward(x)\n",
    "print(x.size() == torch.Size([1, 256, 56, 56]))\n",
    "c = CNNBlock(conv_layer_3, 512, True)\n",
    "x = c.forward(x)\n",
    "print(x.size() == torch.Size([1, 512, 28, 28]))\n",
    "d = CNNBlock(conv_layer_4, 1024, True)\n",
    "x = d.forward(x)\n",
    "print(x.size() == torch.Size([1, 1024, 14, 14]))\n",
    "e = CNNBlock(conv_layer_5, 1024, False)\n",
    "x = e.forward(x)\n",
    "print(x.size() == torch.Size([1, 1024, 7, 7]))\n",
    "f = CNNBlock(conv_layer_6, 1024, False)\n",
    "x = f.forward(x)\n",
    "print(x.size() == torch.Size([1, 1024, 7, 7]))\n",
    "l1 = nn.Linear(7*7*1024,4096)\n",
    "out = nn.Flatten()(x)\n",
    "out = l1(out)\n",
    "out = nn.LeakyReLU(0.1)(out)\n",
    "S=7\n",
    "B=2\n",
    "C=3\n",
    "l2 = nn.Linear(4096,S*S*(B*5 + C))\n",
    "final = l2(out).view(S,S,B*5 + C)\n",
    "print(final)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1539,
   "metadata": {
    "id": "3PJwrvcWW1n7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class YOLO(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=3):\n",
    "        \"\"\"\n",
    "        :param: S * S - количество ячеек на которые разбивается изображение\n",
    "        :param: B - количество предсказанных прямоугольников в каждой ячейке\n",
    "        :param: C - количество классов\n",
    "        \"\"\"\n",
    "        super(YOLO, self).__init__()\n",
    "\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.conv_block_1 = CNNBlock(conv_layer_1, 192, True)\n",
    "        self.conv_block_2 = CNNBlock(conv_layer_2, 256, True)\n",
    "        self.conv_block_3 = CNNBlock(conv_layer_3, 512, True)\n",
    "        self.conv_block_4 = CNNBlock(conv_layer_4, 1024, True)\n",
    "        self.conv_block_5 = CNNBlock(conv_layer_5, 1024, False)\n",
    "        self.conv_block_6 = CNNBlock(conv_layer_6, 1024, False)\n",
    "        self.linear_1 =  nn.Linear(7*7*1024,4096)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "        self.linear_2 =  nn.Linear(4096,S*S*(B*5 + C))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1.forward(x)\n",
    "        x = self.conv_block_2.forward(x)\n",
    "        x = self.conv_block_3.forward(x)\n",
    "        x = self.conv_block_4.forward(x)\n",
    "        x = self.conv_block_5.forward(x)\n",
    "        x = self.conv_block_6.forward(x)\n",
    "        x = self.leaky_relu(self.linear_1(nn.Flatten()(x)))\n",
    "\n",
    "        x = self.linear_2(x).view(self.S,self.S,self.B*5 + self.C)\n",
    "        # print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# temp_model = YOLO()\n",
    "# expected_output_shape = temp_model.S * temp_model.S * (5 * temp_model.B + temp_model.C)\n",
    "#\n",
    "# assert temp_model(train_dataset[0][0]).reshape(-1).shape[0] == expected_output_shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1240,
   "outputs": [],
   "source": [
    "# S = 7\n",
    "# for _x in np.linspace(0,1,S+1)[:-1]:\n",
    "#     x_center = _x + (1/(S))/2\n",
    "#     for _y in np.linspace(0,1,S+1)[:-1]:\n",
    "#         y_center = _y + (1/(S))/2\n",
    "# get_cel_cord([0.5,0.5])\n",
    "# final[1,2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1555,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n\n        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n\n        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n\n        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n\n        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n\n        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n\n        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]],\n       grad_fn=<ViewBackward0>)"
     },
     "execution_count": 1555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo = YOLO()\n",
    "pred = yolo.forward(train_dataset.__getitem__(idx=1)[0])\n",
    "pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJIjWKbcYUYe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Реализуйте YoloLoss - 3 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1541,
   "outputs": [],
   "source": [
    "# S × S × (B ∗ 5 + C) tensor     x, y, w, h, and confidence\n",
    "# “responsible” for predicting an object based on which\n",
    "# prediction has the highest current IOU with the ground\n",
    "# truth\n",
    "# x, y, w, h, c,    x, y, w, h, c,   cl, cl, cl ---- pred\n",
    "# x, y, w, h,    cl, cl, cl --- target\n",
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=3):\n",
    "        \"\"\"\n",
    "        :param: S * S - количество ячеек на которые разбивается изображение\n",
    "        :param: B - количество предсказанных прямоугольников в каждой ячейке\n",
    "        :param: C - количество классов\n",
    "        :param: lambda_noobj - константа для обозначения важности ячеек без объектов\n",
    "        :param: objects_dict - константа для обозначения важности ячеек с объектами\n",
    "        :param: objects_dict - словарь\n",
    "         с ключами -  координатами (i,j) ячеек содержащих объекты\n",
    "         с значениями - таргет значениями для этих ячеек в формате [(b_box, class)]\n",
    "        :param: class2vec - словарь\n",
    "         с ключами числовыми метками классов\n",
    "         с значениями one-hot векторами этого класса\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "        self.objects_dict = {}\n",
    "        self.class2vec = {1:[1,0,0],2:[0,1,0],3:[0,0,1]}\n",
    "\n",
    "    def _get_best_prediction(self, predictions,target):\n",
    "        \"\"\"\n",
    "        для всех ячеек в матрице S*S находит \"ответственные\" значения среди В вариантов\n",
    "        :param predictions: - предсказания модели YOLO\n",
    "        :param target:  - целевые значения\n",
    "        :return best_box_mask: torch.Tensor([S,S,B*5+C])best_box_mask - бинарная маска\n",
    "        \"\"\"\n",
    "        predictions = predictions.squeeze(0)\n",
    "        best_box_mask = torch.zeros([self.S,self.S,self.B*5+self.C])\n",
    "\n",
    "        for i in range(self.S):\n",
    "            for j in range(self.S):\n",
    "                best_box_mask[i,j,-self.C:] = 1\n",
    "                if (i,j) not in self.objects_dict.keys():\n",
    "                    best_box_mask[i,j,[4,9]] = 1 # понадобиться когда мы будем уменьшать уверенность для ячеек бкз объекта\n",
    "                    continue\n",
    "                max_iou=0\n",
    "                best_1, best_2 = (0,5)\n",
    "                for k in range(0,5*self.B,5):\n",
    "\n",
    "                    if intersection_over_union(predictions[i,j,k:k+4], self.objects_dict[(i,j)][0][0]) >= max_iou:\n",
    "                        best_1, best_2 = (k,k+5)\n",
    "                        max_iou = intersection_over_union(predictions[i,j,k:k+4], self.objects_dict[(i,j)][0][0])\n",
    "\n",
    "                best_box_mask[i,j,best_1: best_2] = 1\n",
    "\n",
    "        return best_box_mask\n",
    "\n",
    "    def _dict_init(self, target):\n",
    "        \"\"\"\n",
    "        инициализирует словарь objects_dict при помощи значений из target\n",
    "        :param target: - таргет значение\n",
    "        :return: nan\n",
    "        \"\"\"\n",
    "        self.objects_dict = {}\n",
    "        b_target = target[:-1][0]\n",
    "        clfsses = target[-1]\n",
    "        for b_box, clas in zip(b_target, clfsses):\n",
    "            c  = self._get_cel_cord(b_box)\n",
    "            if c not in self.objects_dict.keys():\n",
    "                self.objects_dict[c] = [(b_box,clas)]\n",
    "            else:\n",
    "                self.objects_dict[c] = self.objects_dict[c].append((b_box,clas))\n",
    "        return\n",
    "\n",
    "    def _get_cel_cord(self, bbox):\n",
    "        \"\"\"\n",
    "        по bounding box возвращает номер ячейки куда приходиться центр\n",
    "        :param bbox: окаймляющая рамка (bounding box) целевого значения\n",
    "        :return x, y: int, int - координаты ячейки (её номера)\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        x = self.S - 1\n",
    "        y = self.S-1\n",
    "        for cord in np.linspace(0,1,self.S+1)[:-1]:\n",
    "            if cord > bbox[0]:\n",
    "                x = count\n",
    "                break\n",
    "            count += 1\n",
    "\n",
    "        count = 0\n",
    "        for cord in np.linspace(0,1,self.S+1)[:-1]:\n",
    "            if cord > bbox[1]:\n",
    "                y = count\n",
    "                break\n",
    "            count += 1\n",
    "        return x, y\n",
    "\n",
    "    def _get_masks(self,target):\n",
    "        \"\"\"\n",
    "         Возращает бинарные маски, указывающие на то, есть ли объект в ячейке.\n",
    "         :param target: целевые значения\n",
    "         :return mask_obj: torch.Tensor([S,S,B*5+C]) - маска присутствия центра объекта\n",
    "         :return mask_noobj: torch.Tensor([S,S,B*5+C]) - маска отсутствия центра объекта\n",
    "        \"\"\"\n",
    "        mask_obj = torch.zeros([self.S,self.S,self.B*5+self.C])\n",
    "        mask_noobj = torch.ones([self.S,self.S,self.B*5+self.C])\n",
    "        for i,j in self.objects_dict.keys():\n",
    "            mask_obj[i,j] = 1\n",
    "            mask_noobj[i,j] = 0\n",
    "\n",
    "        return mask_obj,mask_noobj\n",
    "\n",
    "    def _set_target(self):\n",
    "        \"\"\"\n",
    "        возращает target значение в формате тензора [S,S,5+C]\n",
    "        :return new_target:  - torch.Tensor([S,S,5+C]) - target тензор в формате\n",
    "        \"\"\"\n",
    "        new_target = torch.zeros([self.S,self.S,4+self.C])\n",
    "        for i,j in self.objects_dict.keys():\n",
    "            l = list(self.objects_dict[(i,j)][0][0])\n",
    "            vec = self.class2vec[self.objects_dict[(i,j)][0][1][0].item()]\n",
    "            l = l+vec\n",
    "            new_target[i,j,:] = torch.Tensor(l)\n",
    "\n",
    "        return new_target\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        \"\"\"\n",
    "        Считает loss значение для входных предсказаний и целевых значений\n",
    "        :param predictions: torch.Tensor([S,S,B*5+C]) - предсказания\n",
    "        :param target: (b_boxes, classes) - целевые значения\n",
    "        :return answ: torch.Tensor([1]) - значение loss\n",
    "        \"\"\"\n",
    "        self._dict_init(target)\n",
    "        mask_obj, mask_noobj = self._get_masks(target)\n",
    "        predictions = predictions.reshape(-1, self.S, self.S,  self.B * 5 + self.C)\n",
    "        target = self._set_target()\n",
    "        best_box = self._get_best_prediction(predictions, target)\n",
    "\n",
    "        # print(mask_obj)\n",
    "        # print(mask_noobj)\n",
    "        # print(predictions)\n",
    "        # print(target)\n",
    "        # print(best_box)\n",
    "        # X\n",
    "        answ = self.lambda_coord * self.mse((mask_obj*best_box*predictions)[...,[0]], target[...,0]) + \\\n",
    "               self.lambda_coord * self.mse((mask_obj*best_box*predictions)[...,[5]], target[...,0])\n",
    "\n",
    "        # Y\n",
    "        answ += self.lambda_coord * self.mse((mask_obj*best_box*predictions)[...,[1]], target[...,1]) + \\\n",
    "               self.lambda_coord * self.mse((mask_obj*best_box*predictions)[...,[6]], target[...,1])\n",
    "\n",
    "        # W\n",
    "        answ += self.lambda_coord * self.mse(\n",
    "            torch.sign((mask_obj*best_box*predictions)[...,[2]]) * torch.sqrt(\n",
    "                torch.abs((mask_obj*best_box*predictions)[...,[2]])),\n",
    "            torch.sign(target[...,2]) *torch.sqrt(\n",
    "                torch.abs(target[...,2])\n",
    "            ))+\\\n",
    "            self.lambda_coord * self.mse(\n",
    "            torch.sign((mask_obj*best_box*predictions)[...,[7]]) *torch.sqrt(\n",
    "                torch.abs((mask_obj*best_box*predictions)[...,[7]])),\n",
    "            torch.sign(target[...,2]) *torch.sqrt(\n",
    "                torch.abs(target[...,2])\n",
    "            ))\n",
    "\n",
    "        # H\n",
    "        answ += self.lambda_coord * self.mse(\n",
    "            torch.sign((mask_obj*best_box*predictions)[...,[3]]) * torch.sqrt(\n",
    "                torch.abs((mask_obj*best_box*predictions)[...,[3]])),\n",
    "            torch.sign(target[...,3]) *torch.sqrt(\n",
    "                torch.abs(target[...,3])\n",
    "            ))+\\\n",
    "            self.lambda_coord * self.mse(\n",
    "            torch.sign((mask_obj*best_box*predictions)[...,[8]]) *torch.sqrt(\n",
    "                torch.abs((mask_obj*best_box*predictions)[...,[8]])),\n",
    "            torch.sign(target[...,3]) *torch.sqrt(\n",
    "                torch.abs(target[...,3])\n",
    "            ))\n",
    "\n",
    "        # C mask_obj\n",
    "        answ += self.mse((mask_obj*best_box*predictions)[...,[4]], torch.ones([self.S,self.S])) + \\\n",
    "               self.mse((mask_obj*best_box*predictions)[...,[9]], torch.ones([self.S,self.S]))\n",
    "\n",
    "        # C mask_noobj\n",
    "        answ += self.lambda_noobj * self.mse((mask_noobj*predictions)[...,[4]], torch.zeros([self.S,self.S])) + \\\n",
    "               self.lambda_noobj * self.mse((mask_noobj*predictions)[...,[9]], torch.zeros([self.S,self.S]))\n",
    "\n",
    "        # Classes\n",
    "        answ += self.mse((mask_obj*predictions)[...,[10,11,12]],\n",
    "                                                target[...,[4,5,6]])\n",
    "        return answ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1553,
   "outputs": [],
   "source": [
    "model_yolo = YOLO()\n",
    "f_loss = YoloLoss()\n",
    "optimizer = torch.optim.Adam(model_yolo.parameters(), lr=1e-3, eps=1e-8 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1554,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred  tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "loss  tensor(nan, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataloader:\n",
    "    # model.train()\n",
    "    y_pred = model_yolo(x)\n",
    "    loss = f_loss.forward(y_pred,y)\n",
    "    loss.backward()\n",
    "    print(\"y_pred \",y_pred)\n",
    "    # optimizer.step()\n",
    "    # optimizer.zero_grad()\n",
    "    print(\"loss \",loss)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1518,
   "outputs": [
    {
     "data": {
      "text/plain": "YOLO(\n  (conv_block_1): CNNBlock(\n    (conv_layers): Sequential(\n      (0): Conv2d(3, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    )\n    (batchnorm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (leakyrelu): LeakyReLU(negative_slope=0.1)\n    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): CNNBlock(\n    (conv_layers): Sequential(\n      (0): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n    (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (leakyrelu): LeakyReLU(negative_slope=0.1)\n    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_3): CNNBlock(\n    (conv_layers): Sequential(\n      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n      (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n      (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n    (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (leakyrelu): LeakyReLU(negative_slope=0.1)\n    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_4): CNNBlock(\n    (conv_layers): Sequential(\n      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n      (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (2): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n      (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n      (5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (6): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n      (7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (8): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n      (9): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n    (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (leakyrelu): LeakyReLU(negative_slope=0.1)\n    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_5): CNNBlock(\n    (conv_layers): Sequential(\n      (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n      (1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (2): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n      (3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    )\n    (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (leakyrelu): LeakyReLU(negative_slope=0.1)\n    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_6): CNNBlock(\n    (conv_layers): Sequential(\n      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n    (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (leakyrelu): LeakyReLU(negative_slope=0.1)\n    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (linear_1): Linear(in_features=50176, out_features=4096, bias=True)\n  (leaky_relu): LeakyReLU(negative_slope=0.1)\n  (linear_2): Linear(in_features=4096, out_features=637, bias=True)\n)"
     },
     "execution_count": 1518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters.is_nan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ1eev1EeNk7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Реализуйте дополнительные функции из статьи - 2 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMF8e6yXU6QV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# bboxes -  [x_min, y_min, x_max, y_max, conf]\n",
    "\n",
    "# “responsible” for predicting an object based on which\n",
    "# prediction has the highest current IOU with the ground\n",
    "# truth\n",
    "\n",
    "def non_max_suppression(bboxes, iou_threshold, threshold):\n",
    "    \"\"\"\n",
    "    Non max suppression for list of hypotheses\n",
    "\n",
    "    :bboxes: List[torch.tensor] list of coordinates of bounding boxes and there conference\n",
    "    :iou_threshold: float threshold for intersection_over_union function\n",
    "\n",
    "    :returns: List[torch.tensor] list of coordinates of bounding boxes\n",
    "    \"\"\"\n",
    "    bboxes = bboxes.sort(lambda b_box: b_box[4])\n",
    "    answ = [bboxes[0]]\n",
    "    for bbox in bboxes[1:]:\n",
    "        for answ_box in answ:\n",
    "            if intersection_over_union(bbox, answ_box) > iou_threshold and bbox[4] >= threshold:\n",
    "                answ.append(bbox)\n",
    "    return answ\n",
    "\n",
    "def mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5):\n",
    "    pass\n",
    "\n",
    "def get_bound_boxes(loader, model, iou_threshold=0.5, threshold=0.4):\n",
    "    ## YOUR CODE\n",
    "    return all_pred_boxes, all_true_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z38hYLM6haDk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Обучите модель и посчитайте метрики для задачи детекции - 2 балла \n",
    "\n",
    "Несмотря на то, что в этом блоке ничего сильно нового для вас не ожидается и за него формально дается лишь два балла - провести обучение очень важно для понимания того, насколько правильно реализована ваша модель и лосс.\n",
    "\n",
    "В процессе обучения будет видно все ли размерности совпадают, падает ли лосс и растут ли метрики целевой задачи, поэтому на практике этот пункт гораздо оказывается гораздо важнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1288,
   "metadata": {
    "id": "6BTNHNqtMpSi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class YOLOLearner(pl.LightningModule):\n",
    "    def __init__(self, model, loss) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx) -> torch.Tensor:\n",
    "        x,y = train_batch\n",
    "        #\n",
    "        # x = x.to(device)\n",
    "        # y = y.to(device)\n",
    "        preds = self.forward(x)\n",
    "        print(preds.size(), len(y))\n",
    "        loss_var = self.model.forward(y)\n",
    "        self.log(\"train_loss\", loss_var, prog_bar=True)\n",
    "        return loss_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1398,
   "metadata": {
    "id": "sRl42I2xMpSi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [1398], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mYOLO\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m loss_function \u001B[38;5;241m=\u001B[39m YoloLoss()\n\u001B[1;32m      3\u001B[0m n_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n",
      "Cell \u001B[0;32mIn [1394], line 19\u001B[0m, in \u001B[0;36mYOLO.__init__\u001B[0;34m(self, S, B, C)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv_block_5 \u001B[38;5;241m=\u001B[39m CNNBlock(conv_layer_5, \u001B[38;5;241m1024\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv_block_6 \u001B[38;5;241m=\u001B[39m CNNBlock(conv_layer_6, \u001B[38;5;241m1024\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_1 \u001B[38;5;241m=\u001B[39m  \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m7\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m7\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m1024\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m4096\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mleaky_relu \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLeakyReLU(\u001B[38;5;241m0.1\u001B[39m)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_2 \u001B[38;5;241m=\u001B[39m  nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;241m4096\u001B[39m,S\u001B[38;5;241m*\u001B[39mS\u001B[38;5;241m*\u001B[39m(B\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m5\u001B[39m \u001B[38;5;241m+\u001B[39m C))\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:101\u001B[0m, in \u001B[0;36mLinear.__init__\u001B[0;34m(self, in_features, out_features, bias, device, dtype)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregister_parameter(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbias\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 101\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset_parameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:107\u001B[0m, in \u001B[0;36mLinear.reset_parameters\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset_parameters\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001B[39;00m\n\u001B[1;32m    105\u001B[0m     \u001B[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001B[39;00m\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001B[39;00m\n\u001B[0;32m--> 107\u001B[0m     \u001B[43minit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkaiming_uniform_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    109\u001B[0m         fan_in, _ \u001B[38;5;241m=\u001B[39m init\u001B[38;5;241m.\u001B[39m_calculate_fan_in_and_fan_out(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight)\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/init.py:412\u001B[0m, in \u001B[0;36mkaiming_uniform_\u001B[0;34m(tensor, a, mode, nonlinearity)\u001B[0m\n\u001B[1;32m    410\u001B[0m bound \u001B[38;5;241m=\u001B[39m math\u001B[38;5;241m.\u001B[39msqrt(\u001B[38;5;241m3.0\u001B[39m) \u001B[38;5;241m*\u001B[39m std  \u001B[38;5;66;03m# Calculate uniform bounds from standard deviation\u001B[39;00m\n\u001B[1;32m    411\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 412\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtensor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muniform_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mbound\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbound\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = YOLO()\n",
    "loss_function = YoloLoss()\n",
    "n_epochs = 10\n",
    "\n",
    "yolo_learner = YOLOLearner(model,loss_function )  ## YOUR CODE\n",
    "\n",
    "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "trainer = pl.Trainer(accelerator=device, max_epochs=n_epochs)\n",
    "\n",
    "trainer.fit(yolo_learner, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1453,
   "outputs": [],
   "source": [
    "def train():\n",
    "    '''\n",
    "    params:\n",
    "        model - torch.nn.Module to be fitted\n",
    "        optimizer - model optimizer\n",
    "        criterion - loss function from torch.nn\n",
    "        train_loader - torch.utils.data.Dataloader with train set\n",
    "        test_loader - torch.utils.data.Dataloader with test set\n",
    "                      (if you wish to validate during training)\n",
    "    '''\n",
    "\n",
    "    model = YOLO()\n",
    "    f_loss = YoloLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    train_loader = train_dataloader\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # train\n",
    "        count = 0\n",
    "\n",
    "        for x,y in train_loader:\n",
    "            model.train()\n",
    "            tr_loss = []\n",
    "            y_pred = model(x)\n",
    "            # print(y_pred)\n",
    "            loss = f_loss.forward(y_pred,y)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            print(loss)\n",
    "            return\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1456,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb7ioohR96vu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Посчитайте метрики задачи детекции на валидационной выборке\n",
    "\n",
    "Попробуйте понять насколько хороши ваши показатели. Если числа кажутся подозрительно низкими - возможно вам стоит перепроверить ваше решение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUnlNeot98un",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_YG71pYMpSi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Визуализируйте предсказанные bounding box'ы для любых пяти картинок из __валидационного__ датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgVdVzvMMpSi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image, targets = next(iter(val_dataset))\n",
    "preds = ## YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpp4jHs0MpSi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "image = torchvision.transform.ToPILImage()(image)\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "for box in targets[0]:\n",
    "    ## YOUR CODE\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])])\n",
    "\n",
    "for box in preds[0]:\n",
    "    ## YOUR CODE\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline='red')\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}